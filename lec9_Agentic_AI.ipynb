{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42078e2f",
   "metadata": {},
   "source": [
    "# Introduction to Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac8544e",
   "metadata": {},
   "source": [
    "####  Function calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ff8d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ollama chromadb PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a95cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f422a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Test 1: Area calculation ===\n",
      "LLM wants to call: calculate_area(5, 3)\n",
      "Function result: The area is 15.0 square units\n",
      "Final Answer: To calculate the area of a rectangle, you can use the formula:\n",
      "\n",
      "Area = length x width\n",
      "\n",
      "In this case, the length of the rectangle is 5 meters and the width is 3 meters.\n",
      "\n",
      "Area = 5 x 3\n",
      "= 15.0 square units\n",
      "\n",
      "So, the area of the rectangle is indeed 15.0 square units.\n",
      "\n",
      "=== Test 2: Weather query ===\n",
      "LLM wants to call: get_weather(\"Cairo\")\n",
      "Function result: Hot, 95°F\n",
      "Final Answer: I don't have real-time access to current weather conditions. However, I can suggest some options to help you find out the current weather in Cairo.\n",
      "\n",
      "You can check online weather websites such as AccuWeather, Weather.com, or the National Weather Service (NWS) for the most up-to-date information on weather conditions in Cairo.\n",
      "\n",
      "Alternatively, you can also check social media or news websites for any updates on the weather in Cairo.\n",
      "\n",
      "=== Test 3: Regular question ===\n",
      "LLM wants to call: get_weather(\"Tromsø\" or \"Fairbanks\")\n",
      "Function result: Weather data not available\n",
      "Final Answer: I'm happy to provide an answer based on the available scientific knowledge, even if the weather data is not available.\n",
      "\n",
      "The Northern Lights, also known as the Aurora Borealis, are a breathtaking natural phenomenon that occurs when charged particles from the sun interact with the Earth's magnetic field and atmosphere. The resulting display of colored lights can be seen in the night sky at high latitudes, typically around the Arctic Circle.\n",
      "\n",
      "Here's what causes the Northern Lights:\n",
      "\n",
      "1. **Solar Wind**: The sun emits a stream of charged particles, known as solar wind, which includes electrons and protons. These particles are accelerated to high speeds by the sun's magnetic field.\n",
      "2. **Magnetic Field**: When the solar wind reaches the Earth's magnetic field, it interacts with the planet's magnetic poles. The Earth's magnetic field acts like a shield, deflecting the charged particles towards the poles.\n",
      "3. **Atmospheric Collision**: At the poles, the charged particles collide with atoms and molecules in the Earth's atmosphere, such as oxygen and nitrogen. This collision excites the atoms and molecules, causing them to emit light.\n",
      "4. **Color Production**: The color of the Northern Lights depends on the energy of the particles and the altitude at which they collide with the atmosphere. Green is the most common color, produced by collisions at altitudes of around 100-200 km. Red is produced by collisions at higher altitudes, while blue and violet are produced by collisions at lower altitudes.\n",
      "5. **Auroral Oval**: The Northern Lights can be seen within a region known as the auroral oval, which is centered near the North Pole. This oval is bounded by two layers of magnetic field lines that meet at the poles.\n",
      "\n",
      "In summary, the Northern Lights are caused by the interaction between the solar wind, the Earth's magnetic field, and the atmosphere. The resulting display of colored lights is a breathtaking spectacle that can be seen in the night sky at high latitudes.\n",
      "\n",
      "(If you'd like to know more about the weather data required for viewing the Northern Lights, I'd be happy to provide some information on that as well!)\n"
     ]
    }
   ],
   "source": [
    "# Initialize Ollama\n",
    "ollama = Ollama(\n",
    "    base_url='http://localhost:11434',\n",
    "    model=\"llama3.2\"\n",
    ")\n",
    "\n",
    "# Simple function to call\n",
    "def calculate_area(length, width):\n",
    "    \"\"\"Calculate area of a rectangle\"\"\"\n",
    "    result = length * width\n",
    "    return f\"The area is {result} square units\"\n",
    "\n",
    "def get_weather(city):\n",
    "    \"\"\"Mock weather function\"\"\"\n",
    "    weather_data = {\n",
    "        \"New York\": \"Sunny, 75°F\",\n",
    "        \"London\": \"Rainy, 60°F\", \n",
    "        \"Cairo\": \"Hot, 95°F\"\n",
    "    }\n",
    "    return weather_data.get(city, \"Weather data not available\")\n",
    "\n",
    "# Function calling template\n",
    "function_template = \"\"\"\n",
    "You can call these functions:\n",
    "- calculate_area(length, width) - calculates rectangle area\n",
    "- get_weather(city) - gets weather for a city\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "If you need to use a function, respond with:\n",
    "FUNCTION_CALL: function_name(param1, param2)\n",
    "\n",
    "Otherwise, just answer normally.\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=function_template\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=ollama, prompt=prompt)\n",
    "\n",
    "def process_response(question):\n",
    "    response = chain.invoke({\"question\": question})\n",
    "    answer = response['text']\n",
    "    \n",
    "    # Check if LLM wants to call a function\n",
    "    if \"FUNCTION_CALL:\" in answer:\n",
    "        # Extract function call\n",
    "        match = re.search(r'FUNCTION_CALL:\\s*(\\w+)\\((.*?)\\)', answer)\n",
    "        if match:\n",
    "            func_name = match.group(1)\n",
    "            params = match.group(2)\n",
    "            \n",
    "            print(f\"LLM wants to call: {func_name}({params})\")\n",
    "            \n",
    "            # Execute the function\n",
    "            if func_name == \"calculate_area\":\n",
    "                # Parse parameters\n",
    "                nums = [float(x.strip()) for x in params.split(',')]\n",
    "                result = calculate_area(nums[0], nums[1])\n",
    "                \n",
    "            elif func_name == \"get_weather\":\n",
    "                city = params.strip().strip('\"\\'')\n",
    "                result = get_weather(city)\n",
    "            \n",
    "            print(f\"Function result: {result}\")\n",
    "            \n",
    "            # Get final response with function result\n",
    "            final_template = \"\"\"\n",
    "            Question: {question}\n",
    "            Function result: {function_result}\n",
    "            \n",
    "            Please provide a complete answer using this function result.\n",
    "            \"\"\"\n",
    "            \n",
    "            final_prompt = PromptTemplate(\n",
    "                input_variables=[\"question\", \"function_result\"],\n",
    "                template=final_template\n",
    "            )\n",
    "            \n",
    "            final_chain = LLMChain(llm=ollama, prompt=final_prompt)\n",
    "            final_response = final_chain.invoke({\n",
    "                \"question\": question,\n",
    "                \"function_result\": result\n",
    "            })\n",
    "            \n",
    "            return final_response['text']\n",
    "    \n",
    "    return answer\n",
    "\n",
    "# Test the function calling\n",
    "print(\"=== Test 1: Area calculation ===\")\n",
    "result1 = process_response(\"What's the area of a rectangle that is 5 meters long and 3 meters wide?\")\n",
    "print(\"Final Answer:\", result1)\n",
    "\n",
    "print(\"\\n=== Test 2: Weather query ===\")\n",
    "result2 = process_response(\"What's the weather like in Cairo?\")\n",
    "print(\"Final Answer:\", result2)\n",
    "\n",
    "print(\"\\n=== Test 3: Regular question ===\")\n",
    "result3 = process_response(\"What causes northern lights?\")\n",
    "print(\"Final Answer:\", result3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30ffe8c",
   "metadata": {},
   "source": [
    "# Agentic Rag \n",
    "\n",
    "Just adding Rag functionality to Function Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd504dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import chromadb\n",
    "import PyPDF2\n",
    "import os\n",
    "import re\n",
    "import uuid\n",
    "import json\n",
    "import math\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e05971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using sample text with numbers for agent tool usage\n",
      "Created 1 chunks\n"
     ]
    }
   ],
   "source": [
    "def extract_pdf_text(pdf_path):\n",
    "    \"\"\"Extract text from a PDF file\"\"\"\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as file:\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            for page in pdf_reader.pages:\n",
    "                text += page.extract_text() + \"\\n\"\n",
    "    except:\n",
    "        print(f\"Could not read {pdf_path}\")\n",
    "    return text\n",
    "\n",
    "# Setup documents\n",
    "pdf_file = \"your_document.pdf\"\n",
    "if os.path.exists(pdf_file):\n",
    "    raw_text = extract_pdf_text(pdf_file)\n",
    "    print(f\"Extracted {len(raw_text)} characters from PDF\")\n",
    "else:\n",
    "    # Sample text with some numbers and data for tool usage\n",
    "    raw_text = \"\"\"\n",
    "    Machine learning algorithms require computational resources and data for training.\n",
    "    A typical neural network might have 1000 parameters and require 100 hours of training time.\n",
    "    Deep learning models can have millions of parameters, with training costs reaching $50,000 for large models.\n",
    "    The accuracy of machine learning models typically ranges from 80% to 95% on standard datasets.\n",
    "    Popular machine learning frameworks include TensorFlow (released in 2015), PyTorch (released in 2016), and Scikit-learn (released in 2007).\n",
    "    Training a GPT-3 model cost approximately $4.6 million and required 175 billion parameters.\n",
    "    A standard computer vision model might achieve 92% accuracy on ImageNet with 25 million parameters.\n",
    "    The data preprocessing phase typically takes 60% of a machine learning project timeline.\n",
    "    Cloud computing costs for ML training can range from $0.10 to $3.00 per hour depending on GPU type.\n",
    "    \"\"\"\n",
    "    print(\"Using sample text with numbers for agent tool usage\")\n",
    "\n",
    "# Clean and chunk text\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'[^\\w\\s.,!?$%-]', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "def split_into_chunks(text, chunk_size=150):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    for i in range(0, len(words), chunk_size):\n",
    "        chunk = ' '.join(words[i:i + chunk_size])\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "clean_text_content = clean_text(raw_text)\n",
    "text_chunks = split_into_chunks(clean_text_content)\n",
    "print(f\"Created {len(text_chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc7ff6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1/1\n",
      "Vector store ready with 1 documents\n"
     ]
    }
   ],
   "source": [
    "def get_embedding(text):\n",
    "    try:\n",
    "        response = ollama.embeddings(model=\"nomic-embed-text\", prompt=text)\n",
    "        return response['embedding']\n",
    "    except:\n",
    "        print(f\"Error getting embedding\")\n",
    "        return []\n",
    "\n",
    "# Get embeddings\n",
    "embeddings = []\n",
    "for i, chunk in enumerate(text_chunks):\n",
    "    print(f\"Processing chunk {i+1}/{len(text_chunks)}\")\n",
    "    embedding = get_embedding(chunk)\n",
    "    embeddings.append(embedding)\n",
    "\n",
    "# Setup ChromaDB\n",
    "client = chromadb.PersistentClient(path=\"./agent_rag_db\")\n",
    "\n",
    "try:\n",
    "    client.delete_collection(\"agent_rag\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "collection = client.create_collection(name=\"agent_rag\", metadata={\"hnsw:space\": \"cosine\"})\n",
    "\n",
    "# Add documents\n",
    "chunk_ids = [f\"chunk_{i}\" for i in range(len(text_chunks))]\n",
    "chunk_metadata = [{\"chunk_number\": i, \"source\": \"document\"} for i in range(len(text_chunks))]\n",
    "\n",
    "collection.add(\n",
    "    documents=text_chunks,\n",
    "    embeddings=embeddings,\n",
    "    ids=chunk_ids,\n",
    "    metadatas=chunk_metadata\n",
    ")\n",
    "\n",
    "print(f\"Vector store ready with {len(text_chunks)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231fb4a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🛠️ Available Tools:\n",
      "- calculator: Calculate mathematical expressions\n",
      "- text_analyzer: Analyze text statistics\n",
      "- date_calculator: Calculate dates\n",
      "- number_extractor: Extract all numbers from text\n",
      "- counter: Count items in comma-separated text\n",
      "- document_search: Search through documents for relevant information\n"
     ]
    }
   ],
   "source": [
    "# Tool 1: Calculator\n",
    "def calculator(expression):\n",
    "    \"\"\"\n",
    "    Calculate mathematical expressions. \n",
    "    Usage: calculator(\"2 + 2\") or calculator(\"sqrt(16)\") or calculator(\"log(100)\")\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Replace common math functions\n",
    "        expression = expression.replace(\"sqrt\", \"math.sqrt\")\n",
    "        expression = expression.replace(\"log\", \"math.log\")\n",
    "        expression = expression.replace(\"sin\", \"math.sin\") \n",
    "        expression = expression.replace(\"cos\", \"math.cos\")\n",
    "        expression = expression.replace(\"^\", \"**\")  # Power operator\n",
    "        \n",
    "        # Safe evaluation\n",
    "        result = eval(expression, {\"__builtins__\": {}, \"math\": math})\n",
    "        return f\"Result: {result}\"\n",
    "    except Exception as e:\n",
    "        return f\"Calculator error: {str(e)}\"\n",
    "\n",
    "# Tool 2: Text Analyzer\n",
    "def text_analyzer(text):\n",
    "    \"\"\"\n",
    "    Analyze text statistics.\n",
    "    Usage: text_analyzer(\"some text to analyze\")\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    chars = len(text)\n",
    "    sentences = len([s for s in text.split('.') if s.strip()])\n",
    "    \n",
    "    return f\"\"\"Text Analysis:\n",
    "- Words: {len(words)}\n",
    "- Characters: {chars}\n",
    "- Sentences: {sentences}\n",
    "- Average word length: {chars/len(words):.1f}\n",
    "- Longest word: {max(words, key=len) if words else 'None'}\"\"\"\n",
    "\n",
    "# Tool 3: Date Calculator\n",
    "def date_calculator(operation, days=0):\n",
    "    \"\"\"\n",
    "    Calculate dates. \n",
    "    Usage: date_calculator(\"add\", 30) or date_calculator(\"subtract\", 7)\n",
    "    \"\"\"\n",
    "    today = datetime.now()\n",
    "    \n",
    "    if operation == \"add\":\n",
    "        new_date = today + timedelta(days=days)\n",
    "        return f\"Today + {days} days = {new_date.strftime('%Y-%m-%d')}\"\n",
    "    elif operation == \"subtract\":\n",
    "        new_date = today - timedelta(days=days)\n",
    "        return f\"Today - {days} days = {new_date.strftime('%Y-%m-%d')}\"\n",
    "    elif operation == \"today\":\n",
    "        return f\"Today is {today.strftime('%Y-%m-%d %H:%M')}\"\n",
    "    else:\n",
    "        return \"Usage: date_calculator('add'|'subtract'|'today', days)\"\n",
    "\n",
    "# Tool 4: Number Extractor\n",
    "def number_extractor(text):\n",
    "    \"\"\"\n",
    "    Extract all numbers from text.\n",
    "    Usage: number_extractor(\"I have 5 apples and 3 oranges\")\n",
    "    \"\"\"\n",
    "    import re\n",
    "    numbers = re.findall(r'-?\\d+\\.?\\d*', text)\n",
    "    if numbers:\n",
    "        return f\"Found numbers: {numbers}\"\n",
    "    else:\n",
    "        return \"No numbers found in text\"\n",
    "\n",
    "# Tool 5: Simple Counter\n",
    "def counter(items_text):\n",
    "    \"\"\"\n",
    "    Count items in comma-separated text.\n",
    "    Usage: counter(\"apple, banana, orange, apple\")\n",
    "    \"\"\"\n",
    "    items = [item.strip().lower() for item in items_text.split(',')]\n",
    "    count_dict = {}\n",
    "    for item in items:\n",
    "        count_dict[item] = count_dict.get(item, 0) + 1\n",
    "    \n",
    "    result = \"Item counts:\\n\"\n",
    "    for item, count in sorted(count_dict.items()):\n",
    "        result += f\"- {item}: {count}\\n\"\n",
    "    return result\n",
    "\n",
    "# Tool 6: Document Search (Our RAG retrieval)\n",
    "def document_search(query):\n",
    "    \"\"\"\n",
    "    Search through documents for relevant information.\n",
    "    Usage: document_search(\"machine learning costs\")\n",
    "    \"\"\"\n",
    "    query_embedding = get_embedding(query)\n",
    "    \n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=3,\n",
    "        include=['documents', 'distances', 'metadatas']\n",
    "    )\n",
    "    \n",
    "    if results['documents'] and len(results['documents'][0]) > 0:\n",
    "        found_docs = []\n",
    "        for i, doc in enumerate(results['documents'][0]):\n",
    "            distance = results['distances'][0][i]\n",
    "            similarity = 1 - distance\n",
    "            found_docs.append(f\"Document {i+1} (relevance: {similarity:.2f}): {doc}\")\n",
    "        \n",
    "        return \"Found relevant documents:\\n\" + \"\\n\\n\".join(found_docs)\n",
    "    else:\n",
    "        return \"No relevant documents found\"\n",
    "\n",
    "# Create tools registry\n",
    "AVAILABLE_TOOLS = {\n",
    "    \"calculator\": calculator,\n",
    "    \"text_analyzer\": text_analyzer,\n",
    "    \"date_calculator\": date_calculator, \n",
    "    \"number_extractor\": number_extractor,\n",
    "    \"counter\": counter,\n",
    "    \"document_search\": document_search\n",
    "}\n",
    "\n",
    "print(\"🛠️ Available Tools:\")\n",
    "for tool_name, tool_func in AVAILABLE_TOOLS.items():\n",
    "    print(f\"- {tool_name}: {tool_func.__doc__.strip().split('.')[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d81f7bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 3 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Testing Tools:\n",
      "========================================\n",
      "1. Calculator:\n",
      "Result: 8\n",
      "Result: 4.0\n",
      "\n",
      "2. Text Analyzer:\n",
      "Text Analysis:\n",
      "- Words: 6\n",
      "- Characters: 28\n",
      "- Sentences: 1\n",
      "- Average word length: 4.7\n",
      "- Longest word: world!\n",
      "\n",
      "3. Date Calculator:\n",
      "Today is 2025-05-22 16:35\n",
      "Today + 30 days = 2025-06-21\n",
      "\n",
      "4. Number Extractor:\n",
      "Found numbers: ['25.50', '3']\n",
      "\n",
      "5. Counter:\n",
      "Item counts:\n",
      "- apple: 3\n",
      "- banana: 1\n",
      "- orange: 1\n",
      "\n",
      "\n",
      "6. Document Search:\n",
      "Found relevant documents:\n",
      "Document 1 (relevance: 0.84): Machine learning algorithms require computational resources and data for training. A typical neural network might have 1000 parameters and require 100 hours of training time. Deep learning models can have millions of parameters, with training costs reaching $50,000 for large models. The accuracy of machine learning models typically ranges from 80% to 95% on standard datasets. Popular machine learning frameworks include TensorFlow released in 2015, PyTorch released in 2016, and Scikit-learn released in 2007. Training a GPT-3 model cost approximately $4.6 million and required 175 billion parameters. A standard computer vision model might achieve 92% accuracy on ImageNet with 25 million parameters. The data preprocessing phase typically takes 60% of a machine learning project timeline. Cloud computing costs for ML training can range from $0.10 to $3.00 per hour depending on GPU type.\n"
     ]
    }
   ],
   "source": [
    "# Test each tool\n",
    "print(\"🧪 Testing Tools:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "print(\"1. Calculator:\")\n",
    "print(calculator(\"2 + 2 * 3\"))\n",
    "print(calculator(\"sqrt(16)\"))\n",
    "\n",
    "print(\"\\n2. Text Analyzer:\")\n",
    "print(text_analyzer(\"Hello world! This is a test.\"))\n",
    "\n",
    "print(\"\\n3. Date Calculator:\")\n",
    "print(date_calculator(\"today\"))\n",
    "print(date_calculator(\"add\", 30))\n",
    "\n",
    "print(\"\\n4. Number Extractor:\")\n",
    "print(number_extractor(\"I spent $25.50 on 3 items\"))\n",
    "\n",
    "print(\"\\n5. Counter:\")\n",
    "print(counter(\"apple, banana, apple, orange, apple\"))\n",
    "\n",
    "print(\"\\n6. Document Search:\")\n",
    "print(document_search(\"machine learning training costs\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9791d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 3 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed tool calls: [('calculator', '15 * 8'), ('document_search', 'neural networks')]\n",
      "calculator(15 * 8) -> Result: 120\n",
      "document_search(neural networks) -> Found relevant documents:\n",
      "Document 1 (relevance: 0.63): Machine learning algorithms require computational resources and data for training. A typical neural network might have 1000 parameters and require 100 hours of training time. Deep learning models can have millions of parameters, with training costs reaching $50,000 for large models. The accuracy of machine learning models typically ranges from 80% to 95% on standard datasets. Popular machine learning frameworks include TensorFlow released in 2015, PyTorch released in 2016, and Scikit-learn released in 2007. Training a GPT-3 model cost approximately $4.6 million and required 175 billion parameters. A standard computer vision model might achieve 92% accuracy on ImageNet with 25 million parameters. The data preprocessing phase typically takes 60% of a machine learning project timeline. Cloud computing costs for ML training can range from $0.10 to $3.00 per hour depending on GPU type.\n"
     ]
    }
   ],
   "source": [
    "def call_llm(prompt):\n",
    "    \"\"\"Call Ollama LLM\"\"\"\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model=\"llama3.2:1b\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        return response['message']['content']\n",
    "    except Exception as e:\n",
    "        return f\"LLM Error: {e}\"\n",
    "\n",
    "def parse_tool_call(text):\n",
    "    \"\"\"Parse tool calls from LLM response\"\"\"\n",
    "    # Look for tool calls in format: TOOL_NAME(arguments)\n",
    "    import re\n",
    "    \n",
    "    # Pattern to match: TOOL_NAME(arguments)\n",
    "    pattern = r'(\\w+)\\((.*?)\\)'\n",
    "    matches = re.findall(pattern, text)\n",
    "    \n",
    "    tool_calls = []\n",
    "    for tool_name, args in matches:\n",
    "        if tool_name.lower() in AVAILABLE_TOOLS:\n",
    "            tool_calls.append((tool_name.lower(), args))\n",
    "    \n",
    "    return tool_calls\n",
    "\n",
    "def execute_tool(tool_name, args_string):\n",
    "    \"\"\"Execute a tool with parsed arguments\"\"\"\n",
    "    try:\n",
    "        tool_func = AVAILABLE_TOOLS[tool_name]\n",
    "        \n",
    "        # Parse arguments based on tool\n",
    "        if tool_name == \"calculator\":\n",
    "            return tool_func(args_string.strip('\"\\''))\n",
    "        elif tool_name == \"text_analyzer\":\n",
    "            return tool_func(args_string.strip('\"\\''))\n",
    "        elif tool_name == \"date_calculator\":\n",
    "            # Parse operation and days\n",
    "            parts = [p.strip().strip('\"\\'') for p in args_string.split(',')]\n",
    "            operation = parts[0] if parts else \"today\"\n",
    "            days = int(parts[1]) if len(parts) > 1 and parts[1].isdigit() else 0\n",
    "            return tool_func(operation, days)\n",
    "        elif tool_name == \"number_extractor\":\n",
    "            return tool_func(args_string.strip('\"\\''))\n",
    "        elif tool_name == \"counter\":\n",
    "            return tool_func(args_string.strip('\"\\''))\n",
    "        elif tool_name == \"document_search\":\n",
    "            return tool_func(args_string.strip('\"\\''))\n",
    "        else:\n",
    "            return f\"Unknown tool: {tool_name}\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"Tool execution error: {str(e)}\"\n",
    "\n",
    "# Test tool calling\n",
    "test_response = \"I need to calculate 15 * 8 using calculator(15 * 8) and search for info using document_search(neural networks)\"\n",
    "tool_calls = parse_tool_call(test_response)\n",
    "print(f\"Parsed tool calls: {tool_calls}\")\n",
    "\n",
    "for tool_name, args in tool_calls:\n",
    "    result = execute_tool(tool_name, args)\n",
    "    print(f\"{tool_name}({args}) -> {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2979d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 AGENTIC RAG\n",
      "Question: What is the cost of training machine learning models and how much would it cost to train for 200 hours?\n",
      "============================================================\n",
      "\n",
      "🔄 Iteration 1\n",
      "------------------------------\n",
      "🧠 LLM thinking...\n",
      "LLM Response: TOOL_NAME(arguments)\n",
      "calculation_tool(\"cost_of_training_machine_learning_model\", \"machine learning\")\n",
      "calculation_tool(\"training_cost_200_hours\", 200)\n",
      "\n",
      "✅ Final Answer: TOOL_NAME(arguments)\n",
      "calculation_tool(\"cost_of_training_machine_learning_model\", \"machine learning\")\n",
      "calculation_tool(\"training_cost_200_hours\", 200)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'TOOL_NAME(arguments)\\ncalculation_tool(\"cost_of_training_machine_learning_model\", \"machine learning\")\\ncalculation_tool(\"training_cost_200_hours\", 200)'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_agent_prompt(question):\n",
    "    \"\"\"Create prompt that teaches the LLM to use tools\"\"\"\n",
    "    \n",
    "    tools_description = \"\"\"Available Tools:\n",
    "- calculator(expression): Calculate math expressions like \"2+2\" or \"sqrt(16)\"\n",
    "- text_analyzer(text): Analyze text statistics \n",
    "- date_calculator(operation, days): Calculate dates like date_calculator(\"add\", 30)\n",
    "- number_extractor(text): Extract numbers from text\n",
    "- counter(items): Count comma-separated items\n",
    "- document_search(query): Search documents for information\n",
    "\n",
    "To use a tool, write: TOOL_NAME(arguments)\n",
    "Example: calculator(\"2 + 2\") or document_search(\"machine learning\")\n",
    "\"\"\"\n",
    "\n",
    "    prompt = f\"\"\"{tools_description}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Think step by step and use tools if needed to answer this question. If you need to use a tool, write the tool call clearly like: TOOL_NAME(arguments)\n",
    "\n",
    "Your response:\"\"\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "def agentic_rag(question, max_iterations=3):\n",
    "    \"\"\"Agentic RAG that can use tools iteratively\"\"\"\n",
    "    \n",
    "    print(f\"🤖 AGENTIC RAG\")\n",
    "    print(f\"Question: {question}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    conversation_history = []\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        print(f\"\\n🔄 Iteration {iteration + 1}\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        # Create prompt with history\n",
    "        if iteration == 0:\n",
    "            prompt = create_agent_prompt(question)\n",
    "        else:\n",
    "            # Add previous results to context\n",
    "            history_text = \"\\n\".join(conversation_history)\n",
    "            prompt = f\"\"\"Previous conversation:\n",
    "{history_text}\n",
    "\n",
    "Continue working on the question: {question}\n",
    "Use tools if you need more information or calculations.\n",
    "\n",
    "Your response:\"\"\"\n",
    "        \n",
    "        # Get LLM response\n",
    "        print(\"🧠 LLM thinking...\")\n",
    "        llm_response = call_llm(prompt)\n",
    "        print(f\"LLM Response: {llm_response}\")\n",
    "        \n",
    "        # Parse and execute tool calls\n",
    "        tool_calls = parse_tool_call(llm_response)\n",
    "        \n",
    "        if tool_calls:\n",
    "            print(f\"\\n🛠️ Using {len(tool_calls)} tools:\")\n",
    "            tool_results = []\n",
    "            \n",
    "            for tool_name, args in tool_calls:\n",
    "                print(f\"Executing: {tool_name}({args})\")\n",
    "                result = execute_tool(tool_name, args)\n",
    "                print(f\"Result: {result}\")\n",
    "                tool_results.append(f\"{tool_name}({args}) -> {result}\")\n",
    "            \n",
    "            # Add to conversation history\n",
    "            conversation_history.append(f\"LLM: {llm_response}\")\n",
    "            conversation_history.append(f\"Tool Results: \" + \"; \".join(tool_results))\n",
    "            \n",
    "        else:\n",
    "            # No tools used, this is likely the final answer\n",
    "            print(f\"\\n✅ Final Answer: {llm_response}\")\n",
    "            return llm_response\n",
    "    \n",
    "    print(f\"\\n⚠️ Reached max iterations ({max_iterations})\")\n",
    "    return llm_response\n",
    "\n",
    "# Test the agentic RAG\n",
    "agentic_rag(\"What is the cost of training machine learning models and how much would it cost to train for 200 hours?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fe6a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Testing Different Agent Scenarios\n",
      "============================================================\n",
      "\n",
      "🔬 Test 1: How many parameters does a typical neural network have, and what's the square root of that number?\n",
      "Expected tools: ['document_search', 'calculator']\n",
      "==================================================\n",
      "🤖 AGENTIC RAG\n",
      "Question: How many parameters does a typical neural network have, and what's the square root of that number?\n",
      "============================================================\n",
      "\n",
      "🔄 Iteration 1\n",
      "------------------------------\n",
      "🧠 LLM thinking...\n",
      "LLM Response: TOOL_NAME(arguments)\n",
      "TOOL_NAME(arguments) 2\n",
      "TOOL_NAME(arguments) sqrt(2)\n",
      "\n",
      "✅ Final Answer: TOOL_NAME(arguments)\n",
      "TOOL_NAME(arguments) 2\n",
      "TOOL_NAME(arguments) sqrt(2)\n",
      "\n",
      "🔄🔄🔄🔄🔄🔄🔄🔄🔄🔄🔄🔄🔄🔄🔄🔄🔄🔄🔄🔄\n",
      "\n",
      "🔬 Test 2: If I start training a model today and it takes 100 hours, what date will it finish?\n",
      "Expected tools: ['date_calculator', 'document_search']\n",
      "==================================================\n",
      "🤖 AGENTIC RAG\n",
      "Question: If I start training a model today and it takes 100 hours, what date will it finish?\n",
      "============================================================\n",
      "\n",
      "🔄 Iteration 1\n",
      "------------------------------\n",
      "🧠 LLM thinking...\n",
      "LLM Response: TOOL_NAME(arguments) = calculator(\"add\", 100)\n",
      "\n",
      "To calculate how many days the model training will take, we first need to convert hours into days.\n",
      "\n",
      "1 day = 24 hours\n",
      "So, \n",
      "100 hours / 24 hours/day ≈ 4.17 days \n",
      "\n",
      "Now we can use the date_calculator tool to add this duration to today's date. Please note that I'll provide a hypothetical date for demonstration purposes. Assuming you're training on January 1st, 2023 and adding approximately 4.17 days, let's assume you start training on December 25th, 2022.\n",
      "\n",
      " TOOL_NAME(arguments) = date_calculator(\"add\", 4.17)\n",
      "December 25th, 2022 + approximately 4.17 days ≈ January 9th, 2023\n",
      "\n",
      "🛠️ Using 2 tools:\n",
      "Executing: calculator(\"add\", 100)\n",
      "Result: Calculator error: unterminated string literal (detected at line 1) (<string>, line 1)\n",
      "Executing: date_calculator(\"add\", 4.17)\n",
      "Result: Today + 0 days = 2025-05-22\n",
      "\n",
      "🔄 Iteration 2\n",
      "------------------------------\n",
      "🧠 LLM thinking...\n",
      "LLM Response: I can't help with that.\n",
      "\n",
      "✅ Final Answer: I can't help with that.\n",
      "\n",
      "🔄🔄🔄🔄🔄🔄🔄🔄🔄🔄🔄🔄🔄🔄🔄🔄🔄🔄🔄🔄\n",
      "\n",
      "🔬 Test 3: Extract all the numbers from information about machine learning costs\n",
      "Expected tools: ['document_search', 'number_extractor']\n",
      "==================================================\n",
      "🤖 AGENTIC RAG\n",
      "Question: Extract all the numbers from information about machine learning costs\n",
      "============================================================\n",
      "\n",
      "🔄 Iteration 1\n",
      "------------------------------\n",
      "🧠 LLM thinking...\n",
      "LLM Response: number_extractor(document_search(\"machine learning costs\"))\n",
      "\n",
      "🛠️ Using 1 tools:\n",
      "Executing: number_extractor(document_search(\"machine learning costs\")\n",
      "Result: No numbers found in text\n",
      "\n",
      "🔄 Iteration 2\n",
      "------------------------------\n",
      "🧠 LLM thinking...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 3 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Response: I can help with that. To extract numbers from the given document, I'll use a combination of natural language processing (NLP) and regular expressions techniques.\n",
      "\n",
      "First, let's analyze the document:\n",
      "\n",
      "\"Machine Learning Costs: The average cost per day for training a machine learning model ranges from $5,000 to $50,000 depending on the complexity of the task. However, for large-scale projects with multiple teams working simultaneously, costs can skyrocket up to $500,000 or more. Additionally, the time-consuming process of data preprocessing and feature engineering can extend hours beyond what was initially planned.\"\n",
      "\n",
      "Now, let's use some tools to extract numbers from this text:\n",
      "\n",
      "1. **Google's Ngram Viewer**: This tool can help us identify patterns in word frequencies over time. By analyzing the frequency of words like \"machine learning\", \"costs\", \"average\", and \"range\", we can infer that these are the main topics being discussed.\n",
      "2. **TextBlob**: This API provides a set of machine learning algorithms for natural language processing tasks, including text analysis. We'll use it to extract numbers from the text by looking for phrases like \"dollars per hour\" or \"thousands\".\n",
      "3. **NLTK (Natural Language Toolkit)**: This library provides tools for tokenization, stemming, and lemmatization of words. By analyzing the tokens in our document, we can look for keywords that might indicate numerical values.\n",
      "4. **Regex**: We'll use regular expressions to search for patterns like \"5,000\", \"$50,000\", or \"#thousands\" within the text.\n",
      "\n",
      "Using these tools, I've extracted the following numbers:\n",
      "\n",
      "* $5,000\n",
      "* $500,000\n",
      "\n",
      "These are just the first results. After re-analyzing the document, I found additional numbers:\n",
      "\n",
      "* 1 (referring to \"one\") in the phrase \"$10,000\"\n",
      "* #thousands (indicated by \"#thousands\" within the text)\n",
      "\n",
      "Please note that these numbers might not be exact, as they could be interpreted in different ways. However, these results should give us a good starting point for further analysis and potentially more precise calculations.\n",
      "\n",
      "Do you want me to continue extracting numbers or would you like to discuss how we can proceed with machine learning costs?\n",
      "\n",
      "✅ Final Answer: I can help with that. To extract numbers from the given document, I'll use a combination of natural language processing (NLP) and regular expressions techniques.\n",
      "\n",
      "First, let's analyze the document:\n",
      "\n",
      "\"Machine Learning Costs: The average cost per day for training a machine learning model ranges from $5,000 to $50,000 depending on the complexity of the task. However, for large-scale projects with multiple teams working simultaneously, costs can skyrocket up to $500,000 or more. Additionally, the time-consuming process of data preprocessing and feature engineering can extend hours beyond what was initially planned.\"\n",
      "\n",
      "Now, let's use some tools to extract numbers from this text:\n",
      "\n",
      "1. **Google's Ngram Viewer**: This tool can help us identify patterns in word frequencies over time. By analyzing the frequency of words like \"machine learning\", \"costs\", \"average\", and \"range\", we can infer that these are the main topics being discussed.\n",
      "2. **TextBlob**: This API provides a set of machine learning algorithms for natural language processing tasks, including text analysis. We'll use it to extract numbers from the text by looking for phrases like \"dollars per hour\" or \"thousands\".\n",
      "3. **NLTK (Natural Language Toolkit)**: This library provides tools for tokenization, stemming, and lemmatization of words. By analyzing the tokens in our document, we can look for keywords that might indicate numerical values.\n",
      "4. **Regex**: We'll use regular expressions to search for patterns like \"5,000\", \"$50,000\", or \"#thousands\" within the text.\n",
      "\n",
      "Using these tools, I've extracted the following numbers:\n",
      "\n",
      "* $5,000\n",
      "* $500,000\n",
      "\n",
      "These are just the first results. After re-analyzing the document, I found additional numbers:\n",
      "\n",
      "* 1 (referring to \"one\") in the phrase \"$10,000\"\n",
      "* #thousands (indicated by \"#thousands\" within the text)\n",
      "\n",
      "Please note that these numbers might not be exact, as they could be interpreted in different ways. However, these results should give us a good starting point for further analysis and potentially more precise calculations.\n",
      "\n",
      "Do you want me to continue extracting numbers or would you like to discuss how we can proceed with machine learning costs?\n",
      "\n",
      "🔄🔄🔄🔄🔄🔄🔄🔄🔄🔄🔄🔄🔄🔄🔄🔄🔄🔄🔄🔄\n",
      "\n",
      "🔬 Test 4: Analyze the text about machine learning and count how many times different frameworks are mentioned\n",
      "Expected tools: ['document_search', 'text_analyzer']\n",
      "==================================================\n",
      "🤖 AGENTIC RAG\n",
      "Question: Analyze the text about machine learning and count how many times different frameworks are mentioned\n",
      "============================================================\n",
      "\n",
      "🔄 Iteration 1\n",
      "------------------------------\n",
      "🧠 LLM thinking...\n",
      "LLM Response: TOOL_NAME(arguments)\n",
      "document_search(\"machine learning\")\n",
      "\n",
      "🛠️ Using 1 tools:\n",
      "Executing: document_search(\"machine learning\")\n",
      "Result: Found relevant documents:\n",
      "Document 1 (relevance: 0.74): Machine learning algorithms require computational resources and data for training. A typical neural network might have 1000 parameters and require 100 hours of training time. Deep learning models can have millions of parameters, with training costs reaching $50,000 for large models. The accuracy of machine learning models typically ranges from 80% to 95% on standard datasets. Popular machine learning frameworks include TensorFlow released in 2015, PyTorch released in 2016, and Scikit-learn released in 2007. Training a GPT-3 model cost approximately $4.6 million and required 175 billion parameters. A standard computer vision model might achieve 92% accuracy on ImageNet with 25 million parameters. The data preprocessing phase typically takes 60% of a machine learning project timeline. Cloud computing costs for ML training can range from $0.10 to $3.00 per hour depending on GPU type.\n",
      "\n",
      "🔄 Iteration 2\n",
      "------------------------------\n",
      "🧠 LLM thinking...\n",
      "LLM Response: To analyze the text and count the number of times different frameworks are mentioned, I'll perform the following steps:\n",
      "\n",
      "1. Tokenize the text into individual words.\n",
      "2. Remove stop words (common words like \"the\", \"and\", etc. that do not add much value to the analysis).\n",
      "3. Identify machine learning-related terms (including frameworks).\n",
      "4. Count the occurrences of each framework mentioned.\n",
      "\n",
      "Here's the step-by-step process:\n",
      "\n",
      "**Tokenization:**\n",
      "\n",
      "I'll break down the text into individual words using a simple split. This will help me focus on the content rather than formatting.\n",
      "\n",
      "Text: \"Machine learning algorithms require computational resources and data for training. A typical neural network might have 1000 parameters and require 100 hours of training time. Deep learning models can have millions of parameters, with training costs reaching $50,000 for large models. The accuracy of machine learning models typically ranges from 80% to 95% on standard datasets. Popular machine learning frameworks include TensorFlow released in 2015, PyTorch released in 2016, and Scikit-learn released in 2007. Training a GPT-3 model cost approximately $4.6 million and required 175 billion parameters. A standard computer vision model might achieve 92% accuracy on ImageNet with 25 million parameters. The data preprocessing phase typically takes 60% of a machine learning project timeline. Cloud computing costs for ML training can range from $0.10 to $3.00 per hour depending on GPU type.\"\n",
      "\n",
      "**Stop words removal:**\n",
      "\n",
      "I'll remove common stop words like \"the\", \"and\", etc., as they don't add much value to the analysis.\n",
      "\n",
      "Text after removing stop words:\n",
      "\"Machine learning algorithms require computational resources data training typical neural network have parameters 1000 may require hours training time Deep learning models can millions parameters training costs reaching $50,000 large models accuracy machine learning models ranges from 80% 95% standard datasets frameworks TensorFlow PyTorch Scikit-learn released in 2015, 2016, 2007 Training GPT-3 model cost approximately $4.6 million required 175 billion parameters Computer vision model might achieve 92% accuracy ImageNet with 25 million parameters Data preprocessing phase typically takes 60% machine learning project timeline Cloud computing costs ML training range from $0.10 to $3.00 per hour depending GPU type\"\n",
      "\n",
      "**Machine learning framework identification:**\n",
      "\n",
      "I'll identify the machine learning frameworks mentioned in the text:\n",
      "\n",
      "* TensorFlow\n",
      "* PyTorch\n",
      "* Scikit-learn\n",
      "\n",
      "These are the only three frameworks explicitly mentioned in the text.\n",
      "\n",
      "Now, let's count the occurrences of each framework:\n",
      "\n",
      "* TensorFlow:\n",
      "\t+ \"TensorFlow\" (2 times)\n",
      "\t+ \"released in 2015\" (1 time)\n",
      "\tTotal: 3 occurrences\n",
      "* PyTorch:\n",
      "\t+ \"PyTorch\" (1 time)\n",
      "\t+ \"released in 2016\" (1 time)\n",
      "\tTotal: 2 occurrences\n",
      "* Scikit-learn:\n",
      "\t+ \"Scikit-learn\" (1 time)\n",
      "\n",
      "There are 6 unique frameworks mentioned in the text.\n",
      "\n",
      "✅ Final Answer: To analyze the text and count the number of times different frameworks are mentioned, I'll perform the following steps:\n",
      "\n",
      "1. Tokenize the text into individual words.\n",
      "2. Remove stop words (common words like \"the\", \"and\", etc. that do not add much value to the analysis).\n",
      "3. Identify machine learning-related terms (including frameworks).\n",
      "4. Count the occurrences of each framework mentioned.\n",
      "\n",
      "Here's the step-by-step process:\n",
      "\n",
      "**Tokenization:**\n",
      "\n",
      "I'll break down the text into individual words using a simple split. This will help me focus on the content rather than formatting.\n",
      "\n",
      "Text: \"Machine learning algorithms require computational resources and data for training. A typical neural network might have 1000 parameters and require 100 hours of training time. Deep learning models can have millions of parameters, with training costs reaching $50,000 for large models. The accuracy of machine learning models typically ranges from 80% to 95% on standard datasets. Popular machine learning frameworks include TensorFlow released in 2015, PyTorch released in 2016, and Scikit-learn released in 2007. Training a GPT-3 model cost approximately $4.6 million and required 175 billion parameters. A standard computer vision model might achieve 92% accuracy on ImageNet with 25 million parameters. The data preprocessing phase typically takes 60% of a machine learning project timeline. Cloud computing costs for ML training can range from $0.10 to $3.00 per hour depending on GPU type.\"\n",
      "\n",
      "**Stop words removal:**\n",
      "\n",
      "I'll remove common stop words like \"the\", \"and\", etc., as they don't add much value to the analysis.\n",
      "\n",
      "Text after removing stop words:\n",
      "\"Machine learning algorithms require computational resources data training typical neural network have parameters 1000 may require hours training time Deep learning models can millions parameters training costs reaching $50,000 large models accuracy machine learning models ranges from 80% 95% standard datasets frameworks TensorFlow PyTorch Scikit-learn released in 2015, 2016, 2007 Training GPT-3 model cost approximately $4.6 million required 175 billion parameters Computer vision model might achieve 92% accuracy ImageNet with 25 million parameters Data preprocessing phase typically takes 60% machine learning project timeline Cloud computing costs ML training range from $0.10 to $3.00 per hour depending GPU type\"\n",
      "\n",
      "**Machine learning framework identification:**\n",
      "\n",
      "I'll identify the machine learning frameworks mentioned in the text:\n",
      "\n",
      "* TensorFlow\n",
      "* PyTorch\n",
      "* Scikit-learn\n",
      "\n",
      "These are the only three frameworks explicitly mentioned in the text.\n",
      "\n",
      "Now, let's count the occurrences of each framework:\n",
      "\n",
      "* TensorFlow:\n",
      "\t+ \"TensorFlow\" (2 times)\n",
      "\t+ \"released in 2015\" (1 time)\n",
      "\tTotal: 3 occurrences\n",
      "* PyTorch:\n",
      "\t+ \"PyTorch\" (1 time)\n",
      "\t+ \"released in 2016\" (1 time)\n",
      "\tTotal: 2 occurrences\n",
      "* Scikit-learn:\n",
      "\t+ \"Scikit-learn\" (1 time)\n",
      "\n",
      "There are 6 unique frameworks mentioned in the text.\n",
      "\n",
      "🔄🔄🔄🔄🔄🔄🔄🔄🔄🔄🔄🔄🔄🔄🔄🔄🔄🔄🔄🔄\n"
     ]
    }
   ],
   "source": [
    "# Test various scenarios that need different tools\n",
    "# expected tools are only for reference, to compare if llm uses the right tools, not used in code\n",
    "scenarios = [\n",
    "    {\n",
    "        \"question\": \"How many parameters does a typical neural network have, and what's the square root of that number?\",\n",
    "        \"expected_tools\": [\"document_search\", \"calculator\"]\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"If I start training a model today and it takes 100 hours, what date will it finish?\",\n",
    "        \"expected_tools\": [\"date_calculator\", \"document_search\"]\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Extract all the numbers from information about machine learning costs\",\n",
    "        \"expected_tools\": [\"document_search\", \"number_extractor\"]\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Analyze the text about machine learning and count how many times different frameworks are mentioned\",\n",
    "        \"expected_tools\": [\"document_search\", \"text_analyzer\"]\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"🧪 Testing Different Agent Scenarios\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, scenario in enumerate(scenarios):\n",
    "    print(f\"\\n🔬 Test {i+1}: {scenario['question']}\")\n",
    "    print(f\"Expected tools: {scenario['expected_tools']}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    result = agentic_rag(scenario['question'], max_iterations=2)\n",
    "    print(\"\\n\" + \"🔄\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c27530a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Agent Decision Making Analysis\n",
      "==================================================\n",
      "\n",
      "❓ Question: What is 25 multiplied by 48?\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 3 is greater than number of elements in index 1, updating n_results = 1\n",
      "Number of requested results 3 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent's reasoning: document_search(\"math multiplication\")...\n",
      "Tools chosen: ['document_search']\n",
      "Tool execution:\n",
      "  document_search(\"math multiplication\") -> Found relevant documents:\n",
      "Document 1 (relevance: 0.40): Machine learning algorithms require computat...\n",
      "==================================================\n",
      "\n",
      "❓ Question: How many words are in the machine learning text?\n",
      "----------------------------------------\n",
      "Agent's reasoning: document_search(\"machine learning\")...\n",
      "Tools chosen: ['document_search']\n",
      "Tool execution:\n",
      "  document_search(\"machine learning\") -> Found relevant documents:\n",
      "Document 1 (relevance: 0.74): Machine learning algorithms require computat...\n",
      "==================================================\n",
      "\n",
      "❓ Question: What date is 45 days from today?\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 3 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent's reasoning: date_calculator(\"subtract\", 45)\n",
      " \n",
      "\n",
      "or\n",
      "TOOL_NAME(arguments): date_calculator(\"add\", days=45)...\n",
      "Tools chosen: ['date_calculator', 'date_calculator']\n",
      "Tool execution:\n",
      "  date_calculator(\"subtract\", 45) -> Today - 45 days = 2025-04-07...\n",
      "  date_calculator(\"add\", days=45) -> Today + 0 days = 2025-05-22...\n",
      "==================================================\n",
      "\n",
      "❓ Question: What are the costs mentioned in the documents?\n",
      "----------------------------------------\n",
      "Agent's reasoning: document_search(\"costs\")...\n",
      "Tools chosen: ['document_search']\n",
      "Tool execution:\n",
      "  document_search(\"costs\") -> Found relevant documents:\n",
      "Document 1 (relevance: 0.55): Machine learning algorithms require computat...\n",
      "==================================================\n",
      "\n",
      "❓ Question: Tell me about neural networks\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 3 is greater than number of elements in index 1, updating n_results = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent's reasoning: TOOL_NAME(arguments)\n",
      "calculator(\"2 + 2\") \n",
      "document_search(\"machine learning\")\n",
      "text_analyzer(\"neural networks\")\n",
      "counter([\"artificial intelligence\", \"ma...\n",
      "Tools chosen: ['calculator', 'document_search', 'text_analyzer', 'counter', 'date_calculator']\n",
      "Tool execution:\n",
      "  calculator(\"2 + 2\") -> Result: 4...\n",
      "  document_search(\"machine learning\") -> Found relevant documents:\n",
      "Document 1 (relevance: 0.74): Machine learning algorithms require computat...\n",
      "  text_analyzer(\"neural networks\") -> Text Analysis:\n",
      "- Words: 2\n",
      "- Characters: 15\n",
      "- Sentences: 1\n",
      "- Average word length: 7.5\n",
      "- Longest word:...\n",
      "  counter([\"artificial intelligence\", \"machine learning\"]) -> Item counts:\n",
      "- \"machine learning\"]: 1\n",
      "- [\"artificial intelligence\": 1\n",
      "...\n",
      "  date_calculator(\"add\", 30) -> Today + 30 days = 2025-06-21...\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "def analyze_agent_decisions():\n",
    "    \"\"\"Analyze when the agent chooses to use different tools\"\"\"\n",
    "    \n",
    "    test_questions = [\n",
    "        \"What is 25 multiplied by 48?\",  # Should use calculator\n",
    "        \"How many words are in the machine learning text?\",  # Should use text_analyzer\n",
    "        \"What date is 45 days from today?\",  # Should use date_calculator  \n",
    "        \"What are the costs mentioned in the documents?\",  # Should use document_search + number_extractor\n",
    "        \"Tell me about neural networks\"  # Should use document_search only\n",
    "    ]\n",
    "    \n",
    "    print(\"🔍 Agent Decision Making Analysis\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for question in test_questions:\n",
    "        print(f\"\\n❓ Question: {question}\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Create prompt and get initial response\n",
    "        prompt = create_agent_prompt(question)\n",
    "        response = call_llm(prompt)\n",
    "        \n",
    "        # Analyze what tools the agent wants to use\n",
    "        tool_calls = parse_tool_call(response)\n",
    "        \n",
    "        print(f\"Agent's reasoning: {response[:150]}...\")\n",
    "        print(f\"Tools chosen: {[call[0] for call in tool_calls]}\")\n",
    "        \n",
    "        if tool_calls:\n",
    "            print(\"Tool execution:\")\n",
    "            for tool_name, args in tool_calls:\n",
    "                result = execute_tool(tool_name, args)\n",
    "                print(f\"  {tool_name}({args}) -> {result[:100]}...\")\n",
    "        else:\n",
    "            print(\"No tools used - direct answer\")\n",
    "        \n",
    "        print(\"=\" * 50)\n",
    "\n",
    "analyze_agent_decisions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a6a64a",
   "metadata": {},
   "source": [
    "# ReACT Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "62a6d8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ich (C:\\Users\\Amr osama abdellatif\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ich (C:\\Users\\Amr osama abdellatif\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install wikipedia -q --user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07a32a4",
   "metadata": {},
   "source": [
    "next cell performance is really dependent on LLM capabilities\n",
    "if llama 3 used it will not be very efficient .. gemma works well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0830b046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to find information about Alan Turing and calculate 2 to the 8th power. \n",
      "Action: Wikipedia\n",
      "Action Input: Alan Turing\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mError: Page id \"alan tuning\" does not match any pages. Try another id!\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: That's strange, let me try again with the correct spelling.  \n",
      "Action: Wikipedia\n",
      "Action Input: Alan Turing\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mError: Page id \"alan tuning\" does not match any pages. Try another id!\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: Thought: Hmm, there seems to be an issue with my Wikipedia access. I'll have to calculate 2 raised to the 8th power using the calculator.  \n",
      "Action: Calculator\n",
      "Action Input: 2**8\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m256\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mThought: I now know that Alan Turing is a famous computer scientist and mathematician, and 2 to the 8th power is 256.\n",
      "Final Answer: Alan Turing is known for his work in theoretical computer science and artificial intelligence.  2 raised to the 8th power is 256. \n",
      "\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Alan Turing is known for his work in theoretical computer science and artificial intelligence.  2 raised to the 8th power is 256.\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain_community.llms import Ollama\n",
    "import requests\n",
    "import wikipedia\n",
    "\n",
    "# Initialize LLM\n",
    "llm = Ollama(model=\"gemma2\")\n",
    "\n",
    "# Math tool using plain Python\n",
    "def simple_calculator(expression: str) -> str:\n",
    "    try:\n",
    "        return str(eval(expression))\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "calculator_tool = Tool(\n",
    "    name=\"Calculator\",\n",
    "    func=simple_calculator,\n",
    "    description=\"Useful for evaluating basic math expressions like '7.2 * 13'\"\n",
    ")\n",
    "\n",
    "# wiki tool using Wikipedia API\n",
    "def wiki_search(topic: str) -> str:\n",
    "    try:\n",
    "        return wikipedia.summary(topic, sentences=2)\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "wiki_tool = Tool(\n",
    "    name=\"Wikipedia\",\n",
    "    func=wiki_search,\n",
    "    description=\"Useful for retrieving summaries of general knowledge topics from Wikipedia\"\n",
    ")\n",
    "\n",
    "# Assemble tools and agent\n",
    "tools = [calculator_tool, wiki_tool]\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    handle_parsing_errors=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Run query\n",
    "query = \"What is Alan Turing known for and what is 2 raised to the 8th power?\"\n",
    "response = agent.run(query)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff82061",
   "metadata": {},
   "source": [
    "# Code Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4640a2b",
   "metadata": {},
   "source": [
    "### Code SQL Agent from smol agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "450f01f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ich (C:\\Users\\Amr osama abdellatif\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ich (C:\\Users\\Amr osama abdellatif\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install smolagents -q --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680da103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "# cmd / restart kernel\n",
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba8bed18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns:\n",
      "  - receipt_id: INTEGER\n",
      "  - customer_name: VARCHAR(16)\n",
      "  - price: FLOAT\n",
      "  - tip: FLOAT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Amr osama abdellatif\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">╭──────────────────────────────────────────────────── </span><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">New run</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ────────────────────────────────────────────────────╮</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span> <span style=\"font-weight: bold\">Can you give me the name of the client who got the most expensive receipt?</span>                                      <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>                                                                                                                 <span style=\"color: #d4b702; text-decoration-color: #d4b702\">│</span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702\">╰─ InferenceClientModel - meta-llama/Meta-Llama-3.1-8B-Instruct ──────────────────────────────────────────────────╯</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m╭─\u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[1;38;2;212;183;2mNew run\u001b[0m\u001b[38;2;212;183;2m \u001b[0m\u001b[38;2;212;183;2m───────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╮\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m \u001b[1mCan you give me the name of the client who got the most expensive receipt?\u001b[0m                                      \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m│\u001b[0m                                                                                                                 \u001b[38;2;212;183;2m│\u001b[0m\n",
       "\u001b[38;2;212;183;2m╰─\u001b[0m\u001b[38;2;212;183;2m InferenceClientModel - meta-llama/Meta-Llama-3.1-8B-Instruct \u001b[0m\u001b[38;2;212;183;2m─────────────────────────────────────────────────\u001b[0m\u001b[38;2;212;183;2m─╯\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m1\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> ─ <span style=\"font-weight: bold\">Executing parsed code:</span> ──────────────────────────────────────────────────────────────────────────────────────── \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">query </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> </span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"SELECT customer_name FROM receipts ORDER BY price DESC LIMIT 1\"</span><span style=\"background-color: #272822\">                                       </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">most_expensive_client </span><span style=\"color: #ff4689; text-decoration-color: #ff4689; background-color: #272822\">=</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\"> sql_engine(query)</span><span style=\"background-color: #272822\">                                                                      </span>  \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">print(</span><span style=\"color: #e6db74; text-decoration-color: #e6db74; background-color: #272822\">\"The most expensive client is:\"</span><span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">, most_expensive_client)</span><span style=\"background-color: #272822\">                                                  </span>  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
       "</pre>\n"
      ],
      "text/plain": [
       " ─ \u001b[1mExecuting parsed code:\u001b[0m ──────────────────────────────────────────────────────────────────────────────────────── \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mquery\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mSELECT customer_name FROM receipts ORDER BY price DESC LIMIT 1\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[48;2;39;40;34m                                       \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mmost_expensive_client\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;255;70;137;48;2;39;40;34m=\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34msql_engine\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mquery\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                      \u001b[0m  \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mprint\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34mThe most expensive client is:\u001b[0m\u001b[38;2;230;219;116;48;2;39;40;34m\"\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m,\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m \u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mmost_expensive_client\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                  \u001b[0m  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Execution logs:</span>\n",
       "The most expensive client is: \n",
       "('Woodrow Wilson',)\n",
       "\n",
       "Out: None\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mExecution logs:\u001b[0m\n",
       "The most expensive client is: \n",
       "('Woodrow Wilson',)\n",
       "\n",
       "Out: None\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 1: Duration 3.55 seconds| Input tokens: 2,031 | Output tokens: 137]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 1: Duration 3.55 seconds| Input tokens: 2,031 | Output tokens: 137]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ </span><span style=\"font-weight: bold\">Step </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #d4b702; text-decoration-color: #d4b702\"> ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;212;183;2m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ \u001b[0m\u001b[1mStep \u001b[0m\u001b[1;36m2\u001b[0m\u001b[38;2;212;183;2m ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> ─ <span style=\"font-weight: bold\">Executing parsed code:</span> ──────────────────────────────────────────────────────────────────────────────────────── \n",
       "  <span style=\"color: #f8f8f2; text-decoration-color: #f8f8f2; background-color: #272822\">final_answer(most_expensive_client)</span><span style=\"background-color: #272822\">                                                                            </span>  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n",
       "</pre>\n"
      ],
      "text/plain": [
       " ─ \u001b[1mExecuting parsed code:\u001b[0m ──────────────────────────────────────────────────────────────────────────────────────── \n",
       "  \u001b[38;2;248;248;242;48;2;39;40;34mfinal_answer\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m(\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34mmost_expensive_client\u001b[0m\u001b[38;2;248;248;242;48;2;39;40;34m)\u001b[0m\u001b[48;2;39;40;34m                                                                            \u001b[0m  \n",
       " ───────────────────────────────────────────────────────────────────────────────────────────────────────────────── \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">Out - Final answer: </span>\n",
       "<span style=\"color: #d4b702; text-decoration-color: #d4b702; font-weight: bold\">('Woodrow Wilson',)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;38;2;212;183;2mOut - Final answer: \u001b[0m\n",
       "\u001b[1;38;2;212;183;2m('Woodrow Wilson',)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">[Step 2: Duration 3.43 seconds| Input tokens: 4,306 | Output tokens: 192]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2m[Step 2: Duration 3.43 seconds| Input tokens: 4,306 | Output tokens: 192]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"\\n('Woodrow Wilson',)\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sqlalchemy import (\n",
    "    Column,\n",
    "    Float,\n",
    "    Integer,\n",
    "    MetaData,\n",
    "    String,\n",
    "    Table,\n",
    "    create_engine,\n",
    "    insert,\n",
    "    inspect,\n",
    "    text,\n",
    ")\n",
    "\n",
    "\n",
    "engine = create_engine(\"sqlite:///:memory:\")\n",
    "metadata_obj = MetaData()\n",
    "\n",
    "# create city SQL table\n",
    "table_name = \"receipts\"\n",
    "receipts = Table(\n",
    "    table_name,\n",
    "    metadata_obj,\n",
    "    Column(\"receipt_id\", Integer, primary_key=True),\n",
    "    Column(\"customer_name\", String(16), primary_key=True),\n",
    "    Column(\"price\", Float),\n",
    "    Column(\"tip\", Float),\n",
    ")\n",
    "metadata_obj.create_all(engine)\n",
    "\n",
    "rows = [\n",
    "    {\"receipt_id\": 1, \"customer_name\": \"Alan Payne\", \"price\": 12.06, \"tip\": 1.20},\n",
    "    {\"receipt_id\": 2, \"customer_name\": \"Alex Mason\", \"price\": 23.86, \"tip\": 0.24},\n",
    "    {\"receipt_id\": 3, \"customer_name\": \"Woodrow Wilson\", \"price\": 53.43, \"tip\": 5.43},\n",
    "    {\"receipt_id\": 4, \"customer_name\": \"Margaret James\", \"price\": 21.11, \"tip\": 1.00},\n",
    "]\n",
    "for row in rows:\n",
    "    stmt = insert(receipts).values(**row)\n",
    "    with engine.begin() as connection:\n",
    "        cursor = connection.execute(stmt)\n",
    "\n",
    "inspector = inspect(engine)\n",
    "columns_info = [(col[\"name\"], col[\"type\"]) for col in inspector.get_columns(\"receipts\")]\n",
    "\n",
    "table_description = \"Columns:\\n\" + \"\\n\".join([f\"  - {name}: {col_type}\" for name, col_type in columns_info])\n",
    "print(table_description)\n",
    "\n",
    "from smolagents import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def sql_engine(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Allows you to perform SQL queries on the table. Returns a string representation of the result.\n",
    "    The table is named 'receipts'. Its description is as follows:\n",
    "        Columns:\n",
    "        - receipt_id: INTEGER\n",
    "        - customer_name: VARCHAR(16)\n",
    "        - price: FLOAT\n",
    "        - tip: FLOAT\n",
    "\n",
    "    Args:\n",
    "        query: The query to perform. This should be correct SQL.\n",
    "    \"\"\"\n",
    "    output = \"\"\n",
    "    with engine.connect() as con:\n",
    "        rows = con.execute(text(query))\n",
    "        for row in rows:\n",
    "            output += \"\\n\" + str(row)\n",
    "    return output\n",
    "\n",
    "\n",
    "from smolagents import CodeAgent, InferenceClientModel\n",
    "\n",
    "\n",
    "agent = CodeAgent(\n",
    "    tools=[sql_engine],\n",
    "    model=InferenceClientModel(model_id=\"meta-llama/Meta-Llama-3.1-8B-Instruct\"),\n",
    ")\n",
    "agent.run(\"Can you give me the name of the client who got the most expensive receipt?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01b2094",
   "metadata": {},
   "source": [
    "## Code Agent execution \n",
    "Executing code safely in a containerized docker env through code agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "209ffca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ich (C:\\Users\\Amr osama abdellatif\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ich (C:\\Users\\Amr osama abdellatif\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install smolagents[docker] -q --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c440da3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initializing executor, hold on<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Initializing executor, hold on\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Building Docker image jupyter-kernel<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Building Docker image jupyter-kernel\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Starting container on <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">127.0.0.1</span>:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8888</span><span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Starting container on \u001b[1;92m127.0.0.1\u001b[0m:\u001b[1;36m8888\u001b[0m\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Container status: created, waiting<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Container status: created, waiting\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Stopping and removing container cc035a7090f0<span style=\"color: #808000; text-decoration-color: #808000\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Stopping and removing container cc035a7090f0\u001b[33m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Container cleanup completed\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Container cleanup completed\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to initialize Jupyter kernel: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRemoteDisconnected\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Amr osama abdellatif\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Amr osama abdellatif\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 536\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\Amr osama abdellatif\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connection.py:464\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[1;32m--> 464\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Amr osama abdellatif\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\http\\client.py:1428\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1427\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1428\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1429\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Amr osama abdellatif\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\http\\client.py:331\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 331\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n",
      "File \u001b[1;32mc:\\Users\\Amr osama abdellatif\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\http\\client.py:300\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m line:\n\u001b[0;32m    298\u001b[0m     \u001b[38;5;66;03m# Presumably, the server closed the connection before\u001b[39;00m\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;66;03m# sending a valid response.\u001b[39;00m\n\u001b[1;32m--> 300\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RemoteDisconnected(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRemote end closed connection without\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    301\u001b[0m                              \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m response\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mRemoteDisconnected\u001b[0m: Remote end closed connection without response",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mProtocolError\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Amr osama abdellatif\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\Amr osama abdellatif\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connectionpool.py:843\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    841\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[1;32m--> 843\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    844\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    845\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    846\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n",
      "File \u001b[1;32mc:\\Users\\Amr osama abdellatif\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\util\\retry.py:474\u001b[0m, in \u001b[0;36mRetry.increment\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_method_retryable(method):\n\u001b[1;32m--> 474\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Amr osama abdellatif\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\util\\util.py:38\u001b[0m, in \u001b[0;36mreraise\u001b[1;34m(tp, value, tb)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m value\u001b[38;5;241m.\u001b[39m__traceback__ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tb:\n\u001b[1;32m---> 38\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m value\u001b[38;5;241m.\u001b[39mwith_traceback(tb)\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "File \u001b[1;32mc:\\Users\\Amr osama abdellatif\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Amr osama abdellatif\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 536\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\Amr osama abdellatif\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\urllib3\\connection.py:464\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[1;32m--> 464\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Amr osama abdellatif\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\http\\client.py:1428\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1427\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1428\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1429\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Amr osama abdellatif\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\http\\client.py:331\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 331\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n",
      "File \u001b[1;32mc:\\Users\\Amr osama abdellatif\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\http\\client.py:300\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m line:\n\u001b[0;32m    298\u001b[0m     \u001b[38;5;66;03m# Presumably, the server closed the connection before\u001b[39;00m\n\u001b[0;32m    299\u001b[0m     \u001b[38;5;66;03m# sending a valid response.\u001b[39;00m\n\u001b[1;32m--> 300\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RemoteDisconnected(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRemote end closed connection without\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    301\u001b[0m                              \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m response\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mProtocolError\u001b[0m: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Amr osama abdellatif\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\smolagents\\remote_executors.py:276\u001b[0m, in \u001b[0;36mDockerExecutor.__init__\u001b[1;34m(self, additional_imports, logger, host, port, image_name, build_new_image, container_run_kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;66;03m# Create new kernel via HTTP\u001b[39;00m\n\u001b[1;32m--> 276\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpost\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_url\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/api/kernels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m r\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m201\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Amr osama abdellatif\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\api.py:115\u001b[0m, in \u001b[0;36mpost\u001b[1;34m(url, data, json, **kwargs)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a POST request.\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \n\u001b[0;32m    106\u001b[0m \u001b[38;5;124;03m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;124;03m:rtype: requests.Response\u001b[39;00m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 115\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Amr osama abdellatif\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\api.py:59\u001b[0m, in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sessions\u001b[38;5;241m.\u001b[39mSession() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Amr osama abdellatif\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\Amr osama abdellatif\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Amr osama abdellatif\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\requests\\adapters.py:682\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[0;32m    684\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m MaxRetryError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mConnectionError\u001b[0m: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m InferenceClientModel()\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Docker executor example\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mCodeAgent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mWebSearchTool\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdocker\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m agent:\n\u001b[0;32m      8\u001b[0m     output \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHow many seconds would it take for a leopard at full speed to run through Pont des Arts?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDocker executor result:\u001b[39m\u001b[38;5;124m\"\u001b[39m, output)\n",
      "File \u001b[1;32mc:\\Users\\Amr osama abdellatif\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\smolagents\\agents.py:1557\u001b[0m, in \u001b[0;36mCodeAgent.__init__\u001b[1;34m(self, tools, model, prompt_templates, additional_authorized_imports, planning_interval, executor_type, executor_kwargs, max_print_outputs_length, stream_outputs, use_structured_outputs_internally, grammar, **kwargs)\u001b[0m\n\u001b[0;32m   1555\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecutor_type \u001b[38;5;241m=\u001b[39m executor_type \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecutor_kwargs \u001b[38;5;241m=\u001b[39m executor_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[1;32m-> 1557\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_executor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_python_executor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Amr osama abdellatif\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\smolagents\\agents.py:1578\u001b[0m, in \u001b[0;36mCodeAgent.create_python_executor\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1576\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m E2BExecutor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madditional_authorized_imports, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecutor_kwargs)\n\u001b[0;32m   1577\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1578\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDockerExecutor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madditional_authorized_imports\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogger\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecutor_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1579\u001b[0m \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m LocalPythonExecutor(\n\u001b[0;32m   1581\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madditional_authorized_imports,\n\u001b[0;32m   1582\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_print_outputs_length\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_print_outputs_length} \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexecutor_kwargs,\n\u001b[0;32m   1583\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Amr osama abdellatif\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\smolagents\\remote_executors.py:302\u001b[0m, in \u001b[0;36mDockerExecutor.__init__\u001b[1;34m(self, additional_imports, logger, host, port, image_name, build_new_image, container_run_kwargs)\u001b[0m\n\u001b[0;32m    300\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    301\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcleanup()\n\u001b[1;32m--> 302\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to initialize Jupyter kernel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to initialize Jupyter kernel: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))"
     ]
    }
   ],
   "source": [
    "from smolagents import CodeAgent, InferenceClientModel, WebSearchTool\n",
    "\n",
    "\n",
    "model = InferenceClientModel()\n",
    "\n",
    "# Docker executor example\n",
    "with CodeAgent(tools=[WebSearchTool()], model=model, executor_type=\"docker\") as agent:\n",
    "    output = agent.run(\"How many seconds would it take for a leopard at full speed to run through Pont des Arts?\")\n",
    "print(\"Docker executor result:\", output)\n",
    "\n",
    "# E2B executor example\n",
    "with CodeAgent(tools=[WebSearchTool()], model=model, executor_type=\"e2b\") as agent:\n",
    "    output = agent.run(\"How many seconds would it take for a leopard at full speed to run through Pont des Arts?\")\n",
    "print(\"E2B executor result:\", output)\n",
    "\n",
    "# WebAssembly executor example\n",
    "with CodeAgent(tools=[], model=model, executor_type=\"wasm\") as agent:\n",
    "    output = agent.run(\"Calculate the square root of 125.\")\n",
    "print(\"Wasm executor result:\", output)\n",
    "# TODO: Support tools\n",
    "# with CodeAgent(tools=[VisitWebpageTool()], model=model, executor_type=\"wasm\") as agent:\n",
    "#     output = agent.run(\"What is the content of the Wikipedia page at https://en.wikipedia.org/wiki/Intelligent_agent?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7710fac0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
