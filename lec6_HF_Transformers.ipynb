{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b76ff01",
   "metadata": {},
   "source": [
    "# HF Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4fa919b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install transformers datasets -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d997e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline('sentiment-analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238e5cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier('We are very happy to show you the ðŸ¤— Transformers library.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8833a539",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = classifier([\"We are very happy to show you the ðŸ¤— Transformers library.\",\n",
    "           \"We hope you don't hate it.\"])\n",
    "for result in results:\n",
    "    print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7f7176",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline('sentiment-analysis', model=\"nlptown/bert-base-multilingual-uncased-sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294c1b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949d7612",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "classifier = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a9738f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"We are very happy to show you the ðŸ¤— Transformers library.\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb71776",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "how the tokenizer process two sentences with diff length?\n",
    "Looking at the longest sequence in your batch (the first sentence)\n",
    "Padding the shorter sequences to match that length (not the full 512)\n",
    "truncation=True tells the tokenizer to cut off sequences that exceed the specified max_length.\n",
    "'''\n",
    "\n",
    "## PYTORCH CODE\n",
    "pt_batch = tokenizer(\n",
    "    [\"We are very happy to show you the ðŸ¤— Transformers library.\", \"We hope you don't hate it.\"],\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "pt_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7435d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PYTORCH CODE\n",
    "pt_batch = tokenizer(\n",
    "    [\"We are very happy to show you the ðŸ¤— Transformers library.\", \"We hope you don't hate it.\"],\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "pt_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6782fe91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Import the required libraries\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2caaa94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Load a pre-trained model and tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Prepare input text\n",
    "text = \"Hello, I'm a sentence that needs embedding.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd324fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Tokenize the input\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "print(\"Input IDs shape:\", inputs[\"input_ids\"].shape)\n",
    "print(\"Input IDs:\", inputs[\"input_ids\"])\n",
    "print(\"Attention mask:\", inputs[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45417760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show how embeddings work\n",
    "token_embeddings = model.embeddings.word_embeddings(inputs[\"input_ids\"])\n",
    "print(f\"Token embeddings shape: {token_embeddings.shape}\")\n",
    "\n",
    "# Explain positional encodings - crucial for transformers\n",
    "position_ids = torch.arange(0, inputs[\"input_ids\"].shape[1]).unsqueeze(0)\n",
    "position_embeddings = model.embeddings.position_embeddings(position_ids)\n",
    "print(f\"Position embeddings shape: {position_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dc34de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how tokenizers handle unknown tokens OOV\n",
    "text_with_unknown = \"This contains a very unusual word like supercalifragilisticexpialidocious!\"\n",
    "tokens = tokenizer.tokenize(text_with_unknown)\n",
    "print(\"Tokenized into:\", tokens)\n",
    "print(\"Notice how unusual words are broken into subwords!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4c28d8",
   "metadata": {},
   "source": [
    "#### downstream tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595be798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Classification\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "result = classifier(\"I've been waiting for a HuggingFace course my whole life.\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53f9e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Named Entity Recognition (NER)\n",
    "ner = pipeline(\"ner\")\n",
    "result = ner(\"Hugging Face was founded in Paris, France.\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dfa87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question Answering\n",
    "qa = pipeline(\"question-answering\")\n",
    "result = qa(\n",
    "    question=\"Where was Hugging Face founded?\",\n",
    "    context=\"Hugging Face was founded in Paris, France.\"\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79229158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Generation\n",
    "generator = pipeline(\"text-generation\")\n",
    "result = generator(\"Hugging Face is\", max_length=50, do_sample=True)\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87231083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Summarization\n",
    "summarizer = pipeline(\"summarization\")\n",
    "result = summarizer(\"\"\"\n",
    "    America has changed dramatically during recent years. Not only has the number\n",
    "    of graduates in traditional engineering disciplines such as mechanical, civil,\n",
    "    electrical, chemical, and aeronautical engineering declined, but in most of\n",
    "    the premier American universities engineering curricula now concentrate on\n",
    "    and encourage largely the study of engineering science.\n",
    "\"\"\")\n",
    "print(result[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f94a04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translation\n",
    "translator = pipeline(\"translation_en_to_fr\")\n",
    "result = translator(\"Hugging Face is a technology company based in New York.\")\n",
    "print(result[0]['translation_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efc648a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill-mask (Masked Language Modeling)\n",
    "unmasker = pipeline(\"fill-mask\")\n",
    "result = unmasker(\"Hugging Face is working on <mask> models.\")\n",
    "for res in result:\n",
    "    print(f\"Token: {res['token_str']}, Score: {res['score']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f68f2e",
   "metadata": {},
   "source": [
    "## Lets infer a model from the Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c73046",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "\"microsoft/Phi-3-mini-4k-instruct\",\n",
    "device_map=\"cuda\",\n",
    "torch_dtype=\"auto\",\n",
    "trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7b2b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "return_full_text > By setting this to False , the prompt will not be returned but\n",
    "merely the output of the model.\n",
    "max_new_tokens > The maximum number of tokens the model will generate. By\n",
    "setting a limit, we prevent long and unwieldy output as some\n",
    "models might continue generating output until they reach their context window.\n",
    "do_sample > Whether the model uses a sampling strategy to choose the\n",
    "next token. By setting this to False , the model will always\n",
    "select the next most probable token'''\n",
    "\n",
    "from transformers import pipeline\n",
    "# Create a pipeline\n",
    "generator = pipeline(\n",
    "\"text-generation\",\n",
    "model=model,\n",
    "tokenizer=tokenizer,\n",
    "return_full_text=False,\n",
    "max_new_tokens=500,\n",
    "do_sample=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4408f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Create pipeline\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=500,\n",
    "    do_sample=False\n",
    ")\n",
    "\n",
    "# Define system message and user prompt\n",
    "system_message = \"\"\"You are a helpful, harmless, and precise AI assistant that provides accurate information and never makes things up.\"\"\"\n",
    "user_prompt = \"Tell me about quantum computing\"\n",
    "\n",
    "# Format using chat template\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "# Generate response\n",
    "response = generator(prompt)\n",
    "print(response[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c80c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = generator(\"what is egypt's capital ?\")\n",
    "print(response[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c824705",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Amr osama abdellatif\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'4.44.0'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import transformers\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a76dc5",
   "metadata": {},
   "source": [
    "#### Using a System msg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6e2428",
   "metadata": {},
   "source": [
    "detailed approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5780be05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    device_map=\"cuda\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")\n",
    "\n",
    "# Define a custom system message\n",
    "system_message = \"\"\"You are a helpful, harmless, and precise AI assistant that provides accurate information and never makes things up.\"\"\"\n",
    "\n",
    "# Create messages in the chat format\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": \"Tell me about quantum computing\"}\n",
    "]\n",
    "\n",
    "# Format the messages using the model's chat template\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "print(prompt)\n",
    "# Generate response\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    max_new_tokens=500,\n",
    "    do_sample=False\n",
    ")\n",
    "\n",
    "# Decode and print response\n",
    "response = tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76701453",
   "metadata": {},
   "source": [
    "Too Much Details !! pipeline can handle this details without seperating each component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f24d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Create pipeline\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    return_full_text=False,\n",
    "    max_new_tokens=500,\n",
    "    do_sample=False\n",
    ")\n",
    "\n",
    "# Define system message and user prompt\n",
    "system_message = \"\"\"You are a helpful, harmless, and precise AI assistant that provides accurate information and never makes things up.\"\"\"\n",
    "user_prompt = \"Tell me about quantum computing\"\n",
    "\n",
    "# Format using chat template\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "]\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "# Generate response\n",
    "response = generator(prompt)\n",
    "print(response[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2271e8f2",
   "metadata": {},
   "source": [
    "# Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20c8298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install datasets==3.3.2 -q\n",
    "# restart session collab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e956a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# Load tokenizer and model\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# Tokenize dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=500,\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    ")\n",
    "\n",
    "# Fine-tune model\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
