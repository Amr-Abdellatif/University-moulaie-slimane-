{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "195c8cb1",
   "metadata": {},
   "source": [
    "## Naive Rag : dense retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec1fa6d",
   "metadata": {},
   "source": [
    "### lets use ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2956a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ollama chromadb PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25f1cfa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Amr osama abdellatif\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\onnxruntime\\capi\\onnxruntime_validation.py:26: UserWarning: Unsupported Windows version (11). ONNX Runtime supports Windows 10 and above, only.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "import chromadb\n",
    "import PyPDF2\n",
    "import os\n",
    "import re\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9208b767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 39487 characters from PDF\n"
     ]
    }
   ],
   "source": [
    "def extract_pdf_text(pdf_path):\n",
    "    \"\"\"Extract text from a PDF file - simple version\"\"\"\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as file:\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            for page in pdf_reader.pages:\n",
    "                text += page.extract_text() + \"\\n\"\n",
    "    except:\n",
    "        print(f\"Could not read {pdf_path}\")\n",
    "    return text\n",
    "\n",
    "pdf_file = \"1706.03762v7.pdf\"  \n",
    "if os.path.exists(pdf_file):\n",
    "    raw_text = extract_pdf_text(pdf_file)\n",
    "    print(f\"Extracted {len(raw_text)} characters from PDF\")\n",
    "else:\n",
    "    # Use sample text for testing\n",
    "    raw_text = \"\"\"\n",
    "    Machine learning is a subset of artificial intelligence that focuses on algorithms that can learn from data.\n",
    "    Deep learning uses neural networks with multiple layers to model complex patterns in data.\n",
    "    Natural language processing enables computers to understand and generate human language.\n",
    "    Computer vision allows machines to interpret and understand visual information from images.\n",
    "    Artificial intelligence is transforming many industries through automation and intelligent decision making.\n",
    "    \"\"\"\n",
    "    print(\"Using sample text since no PDF found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "969530d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 30 chunks\n",
      "\n",
      "Chunk 1: Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and...\n",
      "\n",
      "Chunk 2: fraction of the training costs of the best models from the literature. We show that the Transformer ...\n",
      "\n",
      "Chunk 3: 7 neural networks in particular, have been firmly established as state of the art approaches in sequ...\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Clean up the text\"\"\"\n",
    "    # Remove extra spaces and weird characters\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'[^\\w\\s.,!?-]', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "def split_into_chunks(text, chunk_size=200):\n",
    "    \"\"\"Split text into smaller chunks\"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    \n",
    "    for i in range(0, len(words), chunk_size):\n",
    "        chunk = ' '.join(words[i:i + chunk_size])\n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Clean and split our text\n",
    "clean_text_content = clean_text(raw_text)\n",
    "text_chunks = split_into_chunks(clean_text_content)\n",
    "\n",
    "print(f\"Created {len(text_chunks)} chunks\")\n",
    "for i, chunk in enumerate(text_chunks[:3]):  # Show first 3 chunks\n",
    "    print(f\"\\nChunk {i+1}: {chunk[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c51d69e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting embeddings from Ollama...\n",
      "Processing chunk 1/30\n",
      "Processing chunk 2/30\n",
      "Processing chunk 3/30\n",
      "Processing chunk 4/30\n",
      "Processing chunk 5/30\n",
      "Processing chunk 6/30\n",
      "Processing chunk 7/30\n",
      "Processing chunk 8/30\n",
      "Processing chunk 9/30\n",
      "Processing chunk 10/30\n",
      "Processing chunk 11/30\n",
      "Processing chunk 12/30\n",
      "Processing chunk 13/30\n",
      "Processing chunk 14/30\n",
      "Processing chunk 15/30\n",
      "Processing chunk 16/30\n",
      "Processing chunk 17/30\n",
      "Processing chunk 18/30\n",
      "Processing chunk 19/30\n",
      "Processing chunk 20/30\n",
      "Processing chunk 21/30\n",
      "Processing chunk 22/30\n",
      "Processing chunk 23/30\n",
      "Processing chunk 24/30\n",
      "Processing chunk 25/30\n",
      "Processing chunk 26/30\n",
      "Processing chunk 27/30\n",
      "Processing chunk 28/30\n",
      "Processing chunk 29/30\n",
      "Processing chunk 30/30\n",
      "Got 30 embeddings\n",
      "Each embedding has 768 dimensions\n"
     ]
    }
   ],
   "source": [
    "def get_embedding(text):\n",
    "    \"\"\"Get embedding for one piece of text\"\"\"\n",
    "    try:\n",
    "        response = ollama.embeddings(model=\"nomic-embed-text\", prompt=text)\n",
    "        return response['embedding']\n",
    "    except:\n",
    "        print(f\"Error getting embedding for: {text[:50]}...\")\n",
    "        return []\n",
    "\n",
    "# Get embeddings for all chunks\n",
    "print(\"Getting embeddings from Ollama...\")\n",
    "embeddings = []\n",
    "\n",
    "for i, chunk in enumerate(text_chunks):\n",
    "    print(f\"Processing chunk {i+1}/{len(text_chunks)}\")\n",
    "    embedding = get_embedding(chunk)\n",
    "    embeddings.append(embedding)\n",
    "\n",
    "print(f\"Got {len(embeddings)} embeddings\")\n",
    "print(f\"Each embedding has {len(embeddings[0])} dimensions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1066307e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created ChromaDB collection\n"
     ]
    }
   ],
   "source": [
    "# Create ChromaDB client and collection\n",
    "client = chromadb.PersistentClient(path=\"./simple_chroma_db\")\n",
    "\n",
    "# # Delete collection if it exists (fresh start)\n",
    "# try:\n",
    "#     client.delete_collection(\"simple_rag\")\n",
    "# except:\n",
    "#     pass\n",
    "\n",
    "\n",
    "# Create new collection\n",
    "collection = client.create_collection(\n",
    "    name=\"simple_rag\")\n",
    "\n",
    "print(\"Created ChromaDB collection\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85b2eb6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 30 chunks to ChromaDB\n",
      "Collection now has 30 documents\n"
     ]
    }
   ],
   "source": [
    "# Generate IDs and metadata for each chunk\n",
    "chunk_ids = [f\"chunk_{i}\" for i in range(len(text_chunks))]\n",
    "chunk_metadata = [{\"chunk_number\": i, \"source\": \"document\"} for i in range(len(text_chunks))]\n",
    "\n",
    "# Add everything to ChromaDB\n",
    "collection.add(\n",
    "    documents=text_chunks,\n",
    "    embeddings=embeddings,\n",
    "    ids=chunk_ids,\n",
    "    metadatas=chunk_metadata\n",
    ")\n",
    "\n",
    "print(f\"Added {len(text_chunks)} chunks to ChromaDB\")\n",
    "print(f\"Collection now has {collection.count()} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "799a67ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 3 is greater than number of elements in index 2, updating n_results = 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ids': [['chunk_0', 'chunk_1']], 'distances': [[0.5056214568667006, 0.6276617448829913]], 'metadatas': [[{'chunk_number': 0, 'source': 'document'}, {'chunk_number': 1, 'source': 'document'}]], 'embeddings': None, 'documents': [['Machine learning is a subset of artificial intelligence that focuses on algorithms that can learn from data without being explicitly programmed. There are three main types of machine learning supervised learning uses labeled data to train models, unsupervised learning finds patterns in unlabeled data, and reinforcement learning learns through trial and error with rewards. Deep learning uses neural networks with multiple layers to model complex patterns in data. Each layer learns increasingly abstract features. Natural language processing enables computers to understand and generate human language through techniques like tokenization, parsing, and semantic analysis. Computer vision allows machines to interpret visual information from images using convolutional neural networks and feature detection algorithms. Artificial intelligence is transforming industries through automation, predictive analytics, and intelligent decision making systems. Neural networks are inspired by biological neurons and consist of interconnected nodes that process information through weighted connections. Training a neural network involves adjusting weights', 'through backpropagation to minimize prediction errors on training data.']], 'uris': None, 'data': None, 'included': ['documents', 'distances', 'metadatas']}\n",
      "Search results for: 'What is attention is all you need ?'\n",
      "--------------------------------------------------\n",
      "\n",
      "Result 1 (Similarity: 0.494):\n",
      "Machine learning is a subset of artificial intelligence that focuses on algorithms that can learn from data without being explicitly programmed. There are three main types of machine learning supervised learning uses labeled data to train models, unsupervised learning finds patterns in unlabeled data, and reinforcement learning learns through trial and error with rewards. Deep learning uses neural networks with multiple layers to model complex patterns in data. Each layer learns increasingly abstract features. Natural language processing enables computers to understand and generate human language through techniques like tokenization, parsing, and semantic analysis. Computer vision allows machines to interpret visual information from images using convolutional neural networks and feature detection algorithms. Artificial intelligence is transforming industries through automation, predictive analytics, and intelligent decision making systems. Neural networks are inspired by biological neurons and consist of interconnected nodes that process information through weighted connections. Training a neural network involves adjusting weights\n",
      "\n",
      "Result 2 (Similarity: 0.372):\n",
      "through backpropagation to minimize prediction errors on training data.\n"
     ]
    }
   ],
   "source": [
    "def search_documents(query, n_results=3):\n",
    "    \"\"\"Search for similar documents\"\"\"\n",
    "    # Get embedding for the query\n",
    "    query_embedding = get_embedding(query)\n",
    "    \n",
    "    # Search ChromaDB\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=n_results,\n",
    "        include=['documents', 'distances', 'metadatas']\n",
    "    )\n",
    "    print(results)    \n",
    "    return results\n",
    "    \n",
    "\n",
    "# Test the search\n",
    "test_query = \"What is attention is all you need ?\"\n",
    "search_results = search_documents(test_query)\n",
    "\n",
    "print(f\"Search results for: '{test_query}'\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for i in range(len(search_results['documents'][0])):\n",
    "    doc = search_results['documents'][0][i]\n",
    "    distance = search_results['distances'][0][i]\n",
    "    similarity = 1 - distance  # Convert distance to similarity\n",
    "    \n",
    "    print(f\"\\nResult {i+1} (Similarity: {similarity:.3f}):\")\n",
    "    print(f\"{doc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c897d536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is attention is all you need ?\n",
      "Answer: Based on the contexts provided, it appears that \"attention\" is often mentioned in relation to self-attention mechanisms, which are a type of neural network layer used for various tasks such as processing sequential data (e.g., natural language text) and understanding context.\n",
      "\n",
      "In many of these contexts, attention is described as being necessary or crucial for certain tasks. For example, in Context 1, \"but its application should be just - this is what we are missing , in my opinion . EOS pad The Law will never be perfect , but its application should be just - this is what we are missing , in my opinion .\" suggests that the law (EOS pad) requires attention to be applied. Similarly, in Context 2, \"by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions.\" implies that self-attention is necessary for achieving desired results.\n",
      "\n",
      "In Context 3, \"heads attend to a distant dependency of the verb making, completing the phrase making...more difficult.\" suggests that attention is essential for processing complex dependencies and relationships within sentences.\n",
      "\n",
      "Overall, it seems that attention plays a central role in many tasks involving sequential data and understanding context.\n"
     ]
    }
   ],
   "source": [
    "def generate_answer(query, search_results):\n",
    "    \"\"\"Generate answer using retrieved documents\"\"\"\n",
    "    # Get the documents from search results\n",
    "    documents = search_results['documents'][0]\n",
    "    \n",
    "    # Create context from retrieved documents\n",
    "    context = \"\\n\\n\".join([f\"Context {i+1}: {doc}\" for i, doc in enumerate(documents)])\n",
    "    \n",
    "    # Create the prompt\n",
    "    prompt = f\"\"\"Based on the following contexts, answer the question.\n",
    "    If you can't find the answer in the contexts, say so.\n",
    "\n",
    "Contexts:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    # Get response from Ollama\n",
    "    response = ollama.chat(\n",
    "        model=\"llama3.2:1b\",  # or whatever model you have\n",
    "        messages=[{\"role\": \"system\", \"content\": 'you are a ML expert',\n",
    "                    \"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    \n",
    "    return response['message']['content']\n",
    "\n",
    "# Generate answer for our test query\n",
    "answer = generate_answer(test_query, search_results)\n",
    "\n",
    "print(f\"Question: {test_query}\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3fa05d",
   "metadata": {},
   "source": [
    "Task : add multiple documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19d1f68",
   "metadata": {},
   "source": [
    "## Chain Of Thoughts With Rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb7675a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ollama chromadb PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d181b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import chromadb\n",
    "import PyPDF2\n",
    "import os\n",
    "import re\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "01d578e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using detailed sample text since no PDF found\n"
     ]
    }
   ],
   "source": [
    "def extract_pdf_text(pdf_path):\n",
    "    \"\"\"Extract text from a PDF file - simple version\"\"\"\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as file:\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            for page in pdf_reader.pages:\n",
    "                text += page.extract_text() + \"\\n\"\n",
    "    except:\n",
    "        print(f\"Could not read {pdf_path}\")\n",
    "    return text\n",
    "\n",
    "# Test it (replace with your PDF path or skip if no PDF)\n",
    "pdf_file = \"your_document.pdf\"  # Change this path\n",
    "if os.path.exists(pdf_file):\n",
    "    raw_text = extract_pdf_text(pdf_file)\n",
    "    print(f\"Extracted {len(raw_text)} characters from PDF\")\n",
    "else:\n",
    "    # Use sample text for testing - more detailed for CoT\n",
    "    raw_text = \"\"\"\n",
    "    Machine learning is a subset of artificial intelligence that focuses on algorithms that can learn from data without being explicitly programmed.\n",
    "    There are three main types of machine learning: supervised learning uses labeled data to train models, unsupervised learning finds patterns in unlabeled data, and reinforcement learning learns through trial and error with rewards.\n",
    "    Deep learning uses neural networks with multiple layers to model complex patterns in data. Each layer learns increasingly abstract features.\n",
    "    Natural language processing enables computers to understand and generate human language through techniques like tokenization, parsing, and semantic analysis.\n",
    "    Computer vision allows machines to interpret visual information from images using convolutional neural networks and feature detection algorithms.\n",
    "    Artificial intelligence is transforming industries through automation, predictive analytics, and intelligent decision making systems.\n",
    "    Neural networks are inspired by biological neurons and consist of interconnected nodes that process information through weighted connections.\n",
    "    Training a neural network involves adjusting weights through backpropagation to minimize prediction errors on training data.\n",
    "    \"\"\"\n",
    "    print(\"Using detailed sample text since no PDF found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2c983d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 2 chunks\n",
      "\n",
      "Chunk 1: Machine learning is a subset of artificial intelligence that focuses on algorithms that can learn fr...\n",
      "\n",
      "Chunk 2: through backpropagation to minimize prediction errors on training data....\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Clean up the text\"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'[^\\w\\s.,!?-]', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "def split_into_chunks(text, chunk_size=150):\n",
    "    \"\"\"Split text into smaller chunks - smaller for CoT\"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    \n",
    "    for i in range(0, len(words), chunk_size):\n",
    "        chunk = ' '.join(words[i:i + chunk_size])\n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Clean and split our text\n",
    "clean_text_content = clean_text(raw_text)\n",
    "text_chunks = split_into_chunks(clean_text_content)\n",
    "\n",
    "print(f\"Created {len(text_chunks)} chunks\")\n",
    "for i, chunk in enumerate(text_chunks[:3]):\n",
    "    print(f\"\\nChunk {i+1}: {chunk[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2a61c606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting embeddings from Ollama...\n",
      "Processing chunk 1/2\n",
      "Processing chunk 2/2\n",
      "Got 2 embeddings\n"
     ]
    }
   ],
   "source": [
    "def get_embedding(text):\n",
    "    \"\"\"Get embedding for one piece of text\"\"\"\n",
    "    try:\n",
    "        response = ollama.embeddings(model=\"nomic-embed-text\", prompt=text)\n",
    "        return response['embedding']\n",
    "    except:\n",
    "        print(f\"Error getting embedding for: {text[:50]}...\")\n",
    "        return []\n",
    "\n",
    "# Get embeddings for all chunks\n",
    "print(\"Getting embeddings from Ollama...\")\n",
    "embeddings = []\n",
    "\n",
    "for i, chunk in enumerate(text_chunks):\n",
    "    print(f\"Processing chunk {i+1}/{len(text_chunks)}\")\n",
    "    embedding = get_embedding(chunk)\n",
    "    embeddings.append(embedding)\n",
    "\n",
    "print(f\"Got {len(embeddings)} embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571d00d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created ChromaDB collection for CoT RAG\n"
     ]
    }
   ],
   "source": [
    "# Create ChromaDB client and collection\n",
    "client = chromadb.PersistentClient(path=\"./cot_chroma_db\")\n",
    "\n",
    "# # Delete collection if it exists (fresh start)\n",
    "# try:\n",
    "#     client.delete_collection(\"cot_rag\")\n",
    "# except:\n",
    "#     pass\n",
    "\n",
    "# Create new collection\n",
    "collection = client.create_collection(\n",
    "    name=\"cot_rag\")\n",
    "\n",
    "print(\"Created ChromaDB collection for CoT RAG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fae267da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 2 chunks to ChromaDB\n",
      "Collection now has 2 documents\n"
     ]
    }
   ],
   "source": [
    "# Generate IDs and metadata\n",
    "chunk_ids = [f\"chunk_{i}\" for i in range(len(text_chunks))]\n",
    "chunk_metadata = [{\"chunk_number\": i, \"source\": \"document\"} for i in range(len(text_chunks))]\n",
    "\n",
    "# Add everything to ChromaDB\n",
    "collection.add(\n",
    "    documents=text_chunks,\n",
    "    embeddings=embeddings,\n",
    "    ids=chunk_ids,\n",
    "    metadatas=chunk_metadata\n",
    ")\n",
    "\n",
    "print(f\"Added {len(text_chunks)} chunks to ChromaDB\")\n",
    "print(f\"Collection now has {collection.count()} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2312518a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_documents(query, n_results=5):\n",
    "    \"\"\"Search for relevant documents\"\"\"\n",
    "    query_embedding = get_embedding(query)\n",
    "    \n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=n_results,\n",
    "        include=['documents', 'distances', 'metadatas']\n",
    "    )\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dc04fe9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 5 is greater than number of elements in index 2, updating n_results = 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 relevant documents\n"
     ]
    }
   ],
   "source": [
    "def call_llm(prompt, model=\"llama3.2:1b\"):\n",
    "    \"\"\"Make a single LLM call\"\"\"\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        return response['message']['content']\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "# Test basic functions\n",
    "test_search = search_documents(\"What is machine learning?\")\n",
    "print(f\"Found {len(test_search['documents'][0])} relevant documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c8ca639e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç STEP 1: Analyzing the question...\n",
      "Analysis: Analysis of the question:\n",
      "\n",
      "1. The main topic being asked about is \"deep learning\" and its differences from traditional machine learning.\n",
      "2. The specific aspects or details being requested are the fundamental differences between deep learning and traditional machine learning.\n",
      "3. A most helpful type of answer would be one that provides a clear explanation, definition, comparison, or process-based analysis to facilitate understanding.\n",
      "\n",
      "Breakdown of the question:\n",
      "\n",
      "- \"Deep learning\" refers to a subset of machine learning techniques that use neural networks with multiple layers to learn complex patterns in data. It's an area of artificial intelligence (AI) where computers are trained by mimicking how humans think.\n",
      "- Traditional machine learning, on the other hand, focuses on simpler algorithms and models designed to make predictions or classify data without explicitly learning a pattern through experience.\n",
      "\n",
      "Helpful answer types:\n",
      "\n",
      "1. Definition: A concise definition of deep learning would be \"A type of machine learning that uses neural networks with multiple layers to learn complex patterns in data.\"\n",
      "2. Explanation: An explanation of the differences between deep learning and traditional machine learning could include examples, such as how deep learning can handle high-dimensional data (e.g., images), while traditional machine learning struggles.\n",
      "3. Comparison: A comparison between the strengths and weaknesses of each type of machine learning would be helpful, for instance:\n",
      "* Strengths of deep learning: Handling complex patterns in data efficiently, making predictions with high accuracy.\n",
      "* Weaknesses of deep learning: Difficulty in training on small datasets, limited generalizability to new tasks.\n",
      "4. Process-based analysis: A step-by-step explanation of how deep learning works could include discussions on network architecture (e.g., convolutional neural networks), activation functions, and loss functions.\n",
      "\n",
      "Related concepts:\n",
      "\n",
      "- Understanding the basics of machine learning\n",
      "- Familiarity with AI and its applications\n",
      "- Knowledge of data types (images, text, etc.)\n",
      "- Basic understanding of algorithms\n",
      "\n",
      "Sub-questions to address:\n",
      "\n",
      "1. What is a typical dataset used for training deep learning models?\n",
      "2. How does neural network architecture affect the performance of deep learning models?\n"
     ]
    }
   ],
   "source": [
    "def step1_analyze_question(question):\n",
    "    \"\"\"Step 1: Analyze what the question is really asking\"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"Analyze this question and break it down:\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Please identify:\n",
    "1. What is the main topic being asked about?\n",
    "2. What specific aspects or details are being requested?\n",
    "3. What type of answer would be most helpful (definition, explanation, comparison, process, etc.)?\n",
    "4. Are there any sub-questions or related concepts to address?\n",
    "\n",
    "Provide a clear analysis:\"\"\"\n",
    "\n",
    "    print(\"üîç STEP 1: Analyzing the question...\")\n",
    "    analysis = call_llm(prompt)\n",
    "    print(f\"Analysis: {analysis}\")\n",
    "    return analysis\n",
    "\n",
    "# Test Step 1\n",
    "test_question = \"How does deep learning differ from traditional machine learning?\"\n",
    "question_analysis = step1_analyze_question(test_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6b539a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 5 is greater than number of elements in index 2, updating n_results = 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä STEP 2: Evaluating sources...\n",
      "Source Evaluation: Based on the analysis of the original question and its breakdown, here's an evaluation of each source:\n",
      "\n",
      "1. **Source 1**:\n",
      "\t* Relevance: 6/10 (While it provides some general information about machine learning, it doesn't specifically address the differences between deep learning and traditional machine learning.)\n",
      "\t* Specificity: 4/10 (It mentions supervised, unsupervised, and reinforcement learning, but doesn't provide an in-depth analysis of these concepts or their applications.)\n",
      "\t* Information provided: The source provides a brief overview of machine learning as a subset of AI.\n",
      "\t* Priority: Should be prioritized for its general information on machine learning.\n",
      "2. **Source 2**:\n",
      "\t* Relevance: 9/10 (This source explicitly mentions deep learning and traditional machine learning, providing a clear comparison between the two.)\n",
      "\t* Specificity: 8/10 (It provides detailed explanations of neural networks, backpropagation, and their applications in AI.)\n",
      "\t* Information provided: A comprehensive explanation of how deep learning differs from traditional machine learning.\n",
      "\t* Priority: Should be prioritized for its specific information on deep learning.\n",
      "\n",
      "Based on these evaluations, I recommend prioritizing **Source 2** as the most helpful type of answer to answering the question. This source provides a clear definition, comparison, and process-based analysis that directly addresses the differences between deep learning and traditional machine learning.\n",
      "\n",
      "The recommended answer would be:\n",
      "\n",
      "\"Deep learning is a subset of machine learning that uses neural networks with multiple layers to learn complex patterns in data. It differs from traditional machine learning in its use of neural networks with many interconnected nodes (neurons) to model complex patterns, unlike traditional algorithms which focus on simpler models and more linear relationships. Key differences include the ability of deep learning models to handle high-dimensional data and make accurate predictions, whereas traditional machines may struggle with small datasets or require more straightforward approaches.\"\n",
      "\n",
      "This answer provides a clear explanation, definition, comparison, and process-based analysis that should facilitate understanding for most readers.\n"
     ]
    }
   ],
   "source": [
    "def step2_evaluate_sources(question, question_analysis, search_results):\n",
    "    \"\"\"Step 2: Evaluate which sources are most relevant\"\"\"\n",
    "    \n",
    "    # Prepare sources text\n",
    "    sources_text = \"\"\n",
    "    for i, doc in enumerate(search_results['documents'][0]):\n",
    "        sources_text += f\"Source {i+1}: {doc}\\n\\n\"\n",
    "    \n",
    "    prompt = f\"\"\"Based on the question analysis, evaluate these sources:\n",
    "\n",
    "Original Question: {question}\n",
    "\n",
    "Question Analysis: {question_analysis}\n",
    "\n",
    "Available Sources:\n",
    "{sources_text}\n",
    "\n",
    "For each source, determine:\n",
    "1. How relevant is it to answering the question?\n",
    "2. What specific information does it provide?\n",
    "3. Which sources should be prioritized?\n",
    "4. Are there any gaps in the available information?\n",
    "\n",
    "Provide your source evaluation:\"\"\"\n",
    "\n",
    "    print(\"üìä STEP 2: Evaluating sources...\")\n",
    "    evaluation = call_llm(prompt)\n",
    "    print(f\"Source Evaluation: {evaluation}\")\n",
    "    return evaluation\n",
    "\n",
    "# Test Step 2\n",
    "search_results = search_documents(test_question)\n",
    "source_evaluation = step2_evaluate_sources(test_question, question_analysis, search_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1483eb25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîó STEP 3: Synthesizing information...\n",
      "Information Synthesis: The main difference between deep learning and traditional machine learning lies in their approaches to modeling complex patterns in data.\n",
      "\n",
      "Traditional machine learning relies on simpler algorithms and models that focus on making predictions or classifications without explicitly learning a pattern through experience. This approach is often limited by the availability of large amounts of training data, which can be difficult to obtain for small datasets. In contrast, deep learning uses neural networks with multiple layers to model complex patterns in data, allowing it to handle high-dimensional data and make accurate predictions.\n",
      "\n",
      "The key points that emerge from this analysis are:\n",
      "\n",
      "* Traditional machine learning models rely on simpler algorithms and models, whereas deep learning models use neural networks with many interconnected nodes (neurons) to model complex patterns.\n",
      "* Deep learning can handle high-dimensional data, whereas traditional machine learning may struggle with small datasets or require more straightforward approaches.\n",
      "* The complexity of a problem is reflected in the number of layers and interconnected nodes used in the model.\n",
      "\n",
      "The most relevant sources for this analysis are:\n",
      "\n",
      "* Source 1: Provides general information on machine learning, but does not specifically address the differences between deep learning and traditional machine learning. It also lacks specific explanations and applications of these concepts.\n",
      "* Source 2: Offers a comprehensive explanation of how deep learning differs from traditional machine learning, providing detailed explanations of neural networks, backpropagation, and their applications in AI.\n",
      "\n",
      "The logical flow for presenting this information is as follows:\n",
      "\n",
      "1. Introduction to the problem\n",
      "2. Definition of traditional machine learning and its limitations\n",
      "3. Explanation of deep learning and its differences from traditional machine learning\n",
      "4. Comparison between the strengths and weaknesses of each approach\n",
      "5. Examples or case studies that illustrate these concepts\n",
      "\n",
      "The logical flow for presenting this information can be summarized as follows:\n",
      "\n",
      "1. Provide a brief introduction to the problem and the main question being asked.\n",
      "2. Define traditional machine learning and its limitations, highlighting the challenges it faces when dealing with complex data.\n",
      "3. Explain deep learning in detail, focusing on its unique approach to modeling complex patterns in data.\n",
      "4. Compare the strengths and weaknesses of each approach, discussing how deep learning can handle high-dimensional data and make accurate predictions.\n",
      "5. Provide examples or case studies that illustrate these concepts, using the information provided by Source 2 as a reference.\n",
      "\n",
      "The recommended answer would be:\n",
      "\n",
      "\"Traditional machine learning relies on simpler algorithms and models that focus on making predictions or classifications without explicitly learning a pattern through experience. In contrast, deep learning uses neural networks with multiple layers to model complex patterns in data, allowing it to handle high-dimensional data and make accurate predictions. The key differences between these approaches lie in their complexity, handling of data, and modeling capabilities.\"\n",
      "\n",
      "This answer provides a clear explanation, definition, comparison, and process-based analysis that should facilitate understanding for most readers.\n",
      "\n",
      "Sources:\n",
      "Source 1: Machine learning is a subset of artificial intelligence that focuses on algorithms that can learn from data without being explicitly programmed. There are three main types of machine learning supervised learning uses labeled data to train models, unsupervised learning finds patterns in unlabeled data, and reinforcement learning learns through trial and error with rewards. Deep learning uses neural networks with multiple layers to model complex patterns in data. Each layer learns increasingly abstract features. Natural language processing enables computers to understand and generate human language through techniques like tokenization, parsing, and semantic analysis. Computer vision allows machines to interpret visual information from images using convolutional neural networks and feature detection algorithms. Artificial intelligence is transforming industries through automation, predictive analytics, and intelligent decision making systems. Neural networks are inspired by biological neurons and consist of interconnected nodes that process information through weighted connections. Training a neural network involves adjusting weights\n",
      "\n",
      "Source 2: A type of machine learning that uses neural networks with multiple layers to learn complex patterns in data. It differs from traditional machine learning in its use of neural networks with many interconnected nodes (neurons) to model complex patterns, unlike traditional algorithms which focus on simpler models and more linear relationships. Key differences include the ability of deep learning models to handle high-dimensional data and make accurate predictions, whereas traditional machines may struggle with small datasets or require more straightforward approaches.\"\n",
      "\n",
      "Based on your analysis and evaluation:\n",
      "\n",
      "1. What key points are most relevant to understanding the difference between deep learning and traditional machine learning?\n",
      "2. How do these key points relate to each other?\n",
      "3. What patterns or connections can be identified in this information?\n",
      "4. What is the logical flow for presenting this information?\n",
      "\n",
      "The most relevant sources for this analysis are Source 1, which provides general information on machine learning, but does not specifically address the differences between deep learning and traditional machine learning. However, it also lacks specific explanations and applications of these concepts.\n",
      "\n",
      "To answer your questions:\n",
      "\n",
      "1. Key points that emerge from this analysis include:\n",
      "\t* Traditional machine learning relies on simpler algorithms and models.\n",
      "\t* Deep learning uses neural networks with multiple layers to model complex patterns in data.\n",
      "\t* The complexity of a problem is reflected in the number of layers and interconnected nodes used in the model.\n",
      "2. These key points relate to each other because they highlight the differences between traditional machine learning and deep learning approaches.\n",
      "3. A pattern that can be identified in this information is the relationship between the simplicity or complexity of an approach and its ability to handle complex data.\n",
      "4. The logical flow for presenting this information would involve introducing the main question being asked, defining traditional machine learning, explaining deep learning, comparing the strengths and weaknesses of each approach, and providing examples or case studies that illustrate these concepts.\n",
      "\n",
      "The recommended answer would be:\n",
      "\n",
      "\"Traditional machine learning relies on simpler algorithms and models that focus on making predictions or classifications without explicitly learning a pattern through experience. In contrast, deep learning uses neural networks with multiple layers to model complex patterns in data, allowing it to handle high-dimensional data and make accurate predictions. The key differences between these approaches lie in their complexity, handling of data, and modeling capabilities.\"\n",
      "\n",
      "This answer provides a clear explanation, definition, comparison, and process-based analysis that should facilitate understanding for most readers.\n",
      "\n",
      "Sources:\n",
      "Source 1: Machine learning is a subset of artificial intelligence that focuses on algorithms that can learn from data without being explicitly programmed. There are three main types of machine learning supervised learning uses labeled data to train models, unsupervised learning finds patterns in unlabeled data, and reinforcement learning learns through trial and error with rewards. Deep learning uses neural networks with multiple layers to model complex patterns in data. Each layer learns increasingly abstract features. Natural language processing enables computers to understand and generate human language through techniques like tokenization, parsing, and semantic analysis. Computer vision allows machines to interpret visual information from images using convolutional neural networks and feature detection algorithms. Artificial intelligence is transforming industries through automation, predictive analytics, and intelligent decision making systems. Neural networks are inspired by biological neurons and consist of interconnected nodes that process information through weighted connections. Training a neural network involves adjusting weights\n",
      "\n",
      "Source 2: A type of machine learning that uses neural networks with multiple layers to learn complex patterns in data. It differs from traditional machine learning in its use of neural networks with many interconnected nodes (neurons) to model complex patterns, unlike traditional algorithms which focus on simpler models and more linear relationships. Key differences include the ability of deep learning models to handle high-dimensional data and make accurate predictions, whereas traditional machines may struggle with small datasets or require more straightforward approaches.\"\n"
     ]
    }
   ],
   "source": [
    "def step3_synthesize_information(question, question_analysis, source_evaluation, search_results):\n",
    "    \"\"\"Step 3: Synthesize information from sources\"\"\"\n",
    "    \n",
    "    sources_text = \"\"\n",
    "    for i, doc in enumerate(search_results['documents'][0]):\n",
    "        sources_text += f\"Source {i+1}: {doc}\\n\\n\"\n",
    "    \n",
    "    prompt = f\"\"\"Now synthesize the information to build toward an answer:\n",
    "\n",
    "Question: {question}\n",
    "Question Analysis: {question_analysis}\n",
    "Source Evaluation: {source_evaluation}\n",
    "\n",
    "Sources:\n",
    "{sources_text}\n",
    "\n",
    "Based on your analysis and evaluation:\n",
    "1. What are the key points from the most relevant sources?\n",
    "2. How do these points relate to each other?\n",
    "3. What patterns or connections can you identify?\n",
    "4. What is the logical flow for presenting this information?\n",
    "\n",
    "Provide your information synthesis:\"\"\"\n",
    "\n",
    "    print(\"üîó STEP 3: Synthesizing information...\")\n",
    "    synthesis = call_llm(prompt)\n",
    "    print(f\"Information Synthesis: {synthesis}\")\n",
    "    return synthesis\n",
    "\n",
    "# Test Step 3\n",
    "info_synthesis = step3_synthesize_information(test_question, question_analysis, source_evaluation, search_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1256a470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ STEP 4: Constructing final answer...\n",
      "Final Answer: ## Step 1: Identify the key differences between deep learning and traditional machine learning.\n",
      "The key differences between deep learning and traditional machine learning are their approach to handling complex data, the complexity of an approach, and the modeling capabilities.\n",
      "\n",
      "## Step 2: Determine the relevance of each point in understanding the difference between deep learning and traditional machine learning.\n",
      "Traditional machine learning relies on simpler algorithms and models that focus on making predictions or classifications without explicitly learning a pattern through experience. In contrast, deep learning uses neural networks with multiple layers to model complex patterns in data.\n",
      "\n",
      "## Step 3: Explain how these key points relate to each other.\n",
      "The simplicity or complexity of an approach is reflected in the number of layers and interconnected nodes used in the model. The ability of a machine learning approach to handle complex data is also related to its modeling capabilities, as more complex models are better suited for handling high-dimensional data.\n",
      "\n",
      "## Step 4: Identify any patterns or connections that can be identified in this information.\n",
      "The relationship between simplicity and complexity, and the relationship between modeling capabilities and the need for multiple layers, illustrate key concepts in machine learning. Additionally, the distinction between deep learning and traditional machine learning highlights their differences in approach.\n",
      "\n",
      "## Step 5: Develop a logical flow to present this information effectively.\n",
      "The logical flow should start with an introduction to traditional machine learning, followed by an explanation of its limitations (simple algorithms, lack of explicit pattern recognition). Then, introduce deep learning as an alternative that can handle complex data. Finally, explain how the complexity and modeling capabilities of deep learning models are different from those of traditional models.\n",
      "\n",
      "## Step 6: Write a clear and concise final answer.\n",
      "Traditional machine learning is limited to simple algorithms that do not explicitly learn patterns from data. In contrast, deep learning uses neural networks with multiple layers to model complex patterns in data. The complexity of these approaches is reflected in the number of layers and interconnected nodes used.\n",
      "\n",
      "## Step 7: Provide a well-structured final answer.\n",
      "Traditional machine learning relies on simpler algorithms that focus on making predictions or classifications without explicitly learning a pattern through experience. Deep learning, on the other hand, uses neural networks with multiple layers to model complex patterns in data, which is more suitable for handling high-dimensional data.\n",
      "\n",
      "The final answer is: $\\boxed{Traditional machine learning relies on simpler algorithms and does not explicitly learn patterns from data, whereas deep learning models handle complex data using neural networks with multiple layers.}$\n"
     ]
    }
   ],
   "source": [
    "def step4_construct_answer(question, question_analysis, source_evaluation, synthesis):\n",
    "    \"\"\"Step 4: Construct the final answer\"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"Now construct the final comprehensive answer:\n",
    "\n",
    "Original Question: {question}\n",
    "\n",
    "Your Previous Analysis:\n",
    "- Question Analysis: {question_analysis}\n",
    "- Source Evaluation: {source_evaluation}  \n",
    "- Information Synthesis: {synthesis}\n",
    "\n",
    "Based on all your previous thinking, provide a complete, well-structured answer that:\n",
    "1. Directly addresses the original question\n",
    "2. Uses the most relevant information you identified\n",
    "3. Follows the logical flow you developed\n",
    "4. Is clear and comprehensive\n",
    "\n",
    "Final Answer:\"\"\"\n",
    "\n",
    "    print(\"‚úÖ STEP 4: Constructing final answer...\")\n",
    "    final_answer = call_llm(prompt)\n",
    "    print(f\"Final Answer: {final_answer}\")\n",
    "    return final_answer\n",
    "\n",
    "# Test Step 4\n",
    "final_answer = step4_construct_answer(test_question, question_analysis, source_evaluation, info_synthesis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369fa723",
   "metadata": {},
   "source": [
    "## Keyword search\n",
    "\n",
    "What is BM25? ü§î\n",
    "\n",
    "BM25 = \"Best Matching 25\" - it's a keyword scoring algorithm that figures out how relevant a document is to your search query.\n",
    "\n",
    "The Simple Goal üéØ\n",
    "BM25's job: Given a search query like \"machine learning\", score every document from 0 to infinity based on how well it matches. Higher score = better match.\n",
    "\n",
    "How BM25 Thinks üß†\n",
    "BM25 asks 3 simple questions about each document:\n",
    "\n",
    "1. Term Frequency (TF) - \"How often does this word appear?\"\n",
    "2. Document Frequency (DF) - \"How rare is this word?\"\n",
    "3. Document Length - \"Is this document too long or too short?\"\n",
    "\n",
    "BM25 Score = TF √ó IDF √ó Length Penalty\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e503c2",
   "metadata": {},
   "source": [
    "BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3f2864",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ollama langchain langchain-community PyPDF2 rank-bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6be8328c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import PyPDF2\n",
    "import os\n",
    "import re\n",
    "from langchain.retrievers import BM25Retriever\n",
    "from langchain.schema import Document\n",
    "from rank_bm25 import BM25Okapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "268ec4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using sample text with good keywords for BM25\n"
     ]
    }
   ],
   "source": [
    "def extract_pdf_text(pdf_path):\n",
    "    \"\"\"Extract text from a PDF file\"\"\"\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as file:\n",
    "            pdf_reader = PyPDF2.PdfReader(file)\n",
    "            for page in pdf_reader.pages:\n",
    "                text += page.extract_text() + \"\\n\"\n",
    "    except:\n",
    "        print(f\"Could not read {pdf_path}\")\n",
    "    return text\n",
    "\n",
    "# Test it\n",
    "pdf_file = \"your_document.pdf\"  # Change this path\n",
    "if os.path.exists(pdf_file):\n",
    "    raw_text = extract_pdf_text(pdf_file)\n",
    "    print(f\"Extracted {len(raw_text)} characters from PDF\")\n",
    "else:\n",
    "    # Sample text with good keywords for BM25 testing\n",
    "    raw_text = \"\"\"\n",
    "    Machine learning algorithms learn patterns from data to make predictions without explicit programming.\n",
    "    Supervised learning uses labeled training data with input-output pairs to train predictive models.\n",
    "    Unsupervised learning discovers hidden patterns and structures in unlabeled data without target variables.\n",
    "    Reinforcement learning agents learn optimal actions through trial and error using reward signals.\n",
    "    Deep learning neural networks have multiple hidden layers for learning complex hierarchical representations.\n",
    "    Convolutional neural networks excel at image recognition tasks using convolution and pooling operations.\n",
    "    Recurrent neural networks process sequential data like text and time series using memory cells.\n",
    "    Natural language processing combines linguistics and machine learning for text understanding and generation.\n",
    "    Computer vision algorithms analyze digital images and videos to extract meaningful information.\n",
    "    Classification algorithms predict discrete categories while regression algorithms predict continuous values.\n",
    "    Clustering algorithms group similar data points together without labeled examples.\n",
    "    Decision trees create interpretable models using if-then rules for classification and regression.\n",
    "    Random forests combine multiple decision trees to improve accuracy and reduce overfitting.\n",
    "    Support vector machines find optimal decision boundaries for classification problems.\n",
    "    \"\"\"\n",
    "    print(\"Using sample text with good keywords for BM25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "59b06d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 2 chunks for BM25\n",
      "\n",
      "Chunk 1: Machine learning algorithms learn patterns from data to make predictions without explicit programming. Supervised learning uses labeled training data with input-output pairs to train predictive models. Unsupervised learning discovers hidden patterns and structures in unlabeled data without target variables. Reinforcement learning agents learn optimal actions through trial and error using reward signals. Deep learning neural networks have multiple hidden layers for learning complex hierarchical representations. Convolutional neural networks excel at image recognition tasks using convolution and pooling operations. Recurrent neural networks process sequential data like text and time series using memory cells. Natural language processing combines linguistics and machine learning\n",
      "\n",
      "Chunk 2: for text understanding and generation. Computer vision algorithms analyze digital images and videos to extract meaningful information. Classification algorithms predict discrete categories while regression algorithms predict continuous values. Clustering algorithms group similar data points together without labeled examples. Decision trees create interpretable models using if-then rules for classification and regression. Random forests combine multiple decision trees to improve accuracy and reduce overfitting. Support vector machines find optimal decision boundaries for classification problems.\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Clean up the text\"\"\"\n",
    "    # Keep more words for BM25 - it needs keywords\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'[^\\w\\s.,!?-]', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "def split_into_chunks(text, chunk_size=100):\n",
    "    \"\"\"Split text into chunks - smaller for better keyword matching\"\"\"\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    \n",
    "    for i in range(0, len(words), chunk_size):\n",
    "        chunk = ' '.join(words[i:i + chunk_size])\n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Clean and split our text\n",
    "clean_text_content = clean_text(raw_text)\n",
    "text_chunks = split_into_chunks(clean_text_content)\n",
    "\n",
    "print(f\"Created {len(text_chunks)} chunks for BM25\")\n",
    "for i, chunk in enumerate(text_chunks[:3]):\n",
    "    print(f\"\\nChunk {i+1}: {chunk}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e2ec2fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 2 LangChain documents\n",
      "Sample document: Machine learning algorithms learn patterns from data to make predictions without explicit programmin...\n",
      "Sample metadata: {'chunk_id': 0, 'source': 'document', 'chunk_number': 0}\n"
     ]
    }
   ],
   "source": [
    "def create_langchain_documents(text_chunks):\n",
    "    \"\"\"Convert text chunks to LangChain Document objects\"\"\"\n",
    "    documents = []\n",
    "    \n",
    "    for i, chunk in enumerate(text_chunks):\n",
    "        doc = Document(\n",
    "            page_content=chunk,\n",
    "            metadata={\n",
    "                \"chunk_id\": i,\n",
    "                \"source\": \"document\",\n",
    "                \"chunk_number\": i\n",
    "            }\n",
    "        )\n",
    "        documents.append(doc)\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# Create LangChain documents\n",
    "langchain_docs = create_langchain_documents(text_chunks)\n",
    "\n",
    "print(f\"Created {len(langchain_docs)} LangChain documents\")\n",
    "print(f\"Sample document: {langchain_docs[0].page_content[:100]}...\")\n",
    "print(f\"Sample metadata: {langchain_docs[0].metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d0270f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BM25 Retriever created!\n",
      "Configured to return top 4 documents\n",
      "Total documents indexed: 2\n"
     ]
    }
   ],
   "source": [
    "# Create BM25 retriever using LangChain\n",
    "bm25_retriever = BM25Retriever.from_documents(langchain_docs)\n",
    "\n",
    "# Set how many documents to retrieve\n",
    "bm25_retriever.k = 4  # Return top 4 most relevant documents\n",
    "\n",
    "print(\"BM25 Retriever created!\")\n",
    "print(f\"Configured to return top {bm25_retriever.k} documents\")\n",
    "print(f\"Total documents indexed: {len(langchain_docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3611f7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç BM25 Search for: 'machine learning algorithms'\n",
      "--------------------------------------------------\n",
      "Found 2 relevant documents:\n",
      "\n",
      "Result 1:\n",
      "Content: Machine learning algorithms learn patterns from data to make predictions without explicit programming. Supervised learning uses labeled training data with input-output pairs to train predictive models. Unsupervised learning discovers hidden patterns and structures in unlabeled data without target variables. Reinforcement learning agents learn optimal actions through trial and error using reward signals. Deep learning neural networks have multiple hidden layers for learning complex hierarchical representations. Convolutional neural networks excel at image recognition tasks using convolution and pooling operations. Recurrent neural networks process sequential data like text and time series using memory cells. Natural language processing combines linguistics and machine learning\n",
      "Metadata: {'chunk_id': 0, 'source': 'document', 'chunk_number': 0}\n",
      "------------------------------\n",
      "\n",
      "Result 2:\n",
      "Content: for text understanding and generation. Computer vision algorithms analyze digital images and videos to extract meaningful information. Classification algorithms predict discrete categories while regression algorithms predict continuous values. Clustering algorithms group similar data points together without labeled examples. Decision trees create interpretable models using if-then rules for classification and regression. Random forests combine multiple decision trees to improve accuracy and reduce overfitting. Support vector machines find optimal decision boundaries for classification problems.\n",
      "Metadata: {'chunk_id': 1, 'source': 'document', 'chunk_number': 1}\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Amr osama abdellatif\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "def test_bm25_search(query):\n",
    "    \"\"\"Test BM25 search with a query\"\"\"\n",
    "    print(f\"üîç BM25 Search for: '{query}'\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Search using BM25\n",
    "    results = bm25_retriever.get_relevant_documents(query)\n",
    "    \n",
    "    print(f\"Found {len(results)} relevant documents:\")\n",
    "    \n",
    "    for i, doc in enumerate(results):\n",
    "        print(f\"\\nResult {i+1}:\")\n",
    "        print(f\"Content: {doc.page_content}\")\n",
    "        print(f\"Metadata: {doc.metadata}\")\n",
    "        print(\"-\" * 30)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test BM25 search\n",
    "test_query = \"machine learning algorithms\"\n",
    "search_results = test_bm25_search(test_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b36d6c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: machine learning algorithms\n",
      "BM25 Answer: Based on the provided sources, machine learning algorithms are categorized into three main types:\n",
      "\n",
      "1. **Supervised Learning**: Uses labeled training data with input-output pairs to train predictive models (Source 1).\n",
      "2. **Unsupervised Learning**: Discovers hidden patterns and structures in unlabeled data without target variables (Source 1).\n",
      "3. **Reinforcement Learning**: Learns optimal actions through trial and error using reward signals, but does not explicitly program the algorithm (Source 1).\n",
      "\n",
      "Additionally, other types of machine learning algorithms mentioned include:\n",
      "\n",
      "* **Deep Learning**: Uses multiple hidden layers for learning complex hierarchical representations (Source 2).\n",
      "* **Convolutional Neural Networks (CNNs)**: Excel at image recognition tasks using convolution and pooling operations (Source 2).\n",
      "* **Recurrent Neural Networks (RNNs)**: Process sequential data like text and time series using memory cells (Source 2).\n",
      "* **Natural Language Processing (NLP)**: Combines linguistics and machine learning for text understanding and generation.\n"
     ]
    }
   ],
   "source": [
    "def call_ollama(prompt):\n",
    "    \"\"\"Simple Ollama call\"\"\"\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model=\"llama3.2:1b\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        return response['message']['content']\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "def generate_answer_from_bm25(query, bm25_results):\n",
    "    \"\"\"Generate answer using BM25 retrieved documents\"\"\"\n",
    "    \n",
    "    # Combine retrieved documents\n",
    "    context = \"\"\n",
    "    for i, doc in enumerate(bm25_results):\n",
    "        context += f\"Source {i+1}: {doc.page_content}\\n\\n\"\n",
    "    \n",
    "    # Create prompt\n",
    "    prompt = f\"\"\"Based on the following sources found through keyword search, answer the question.\n",
    "\n",
    "Sources:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    # Get answer from Ollama\n",
    "    answer = call_ollama(prompt)\n",
    "    return answer\n",
    "\n",
    "# Test answer generation\n",
    "answer = generate_answer_from_bm25(test_query, search_results)\n",
    "print(f\"\\nQuestion: {test_query}\")\n",
    "print(f\"BM25 Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7d550d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù BM25 RAG Pipeline\n",
      "Question: What is supervised learning?\n",
      "============================================================\n",
      "üîç Step 1: BM25 keyword search...\n",
      "üìÑ Step 2: Retrieved documents:\n",
      "  1. for text understanding and generation. Computer vision algorithms analyze digita...\n",
      "  2. Machine learning algorithms learn patterns from data to make predictions without...\n",
      "ü§ñ Step 3: Generating answer...\n",
      "\n",
      "‚úÖ BM25 Answer: Supervised learning is a type of machine learning algorithm that uses labeled training data with input-output pairs to train predictive models. In other words, it involves teaching a model what data to expect as output based on the given inputs. The model learns from the patterns and relationships in the data, allowing it to make accurate predictions or classifications without needing explicit programming.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Supervised learning is a type of machine learning algorithm that uses labeled training data with input-output pairs to train predictive models. In other words, it involves teaching a model what data to expect as output based on the given inputs. The model learns from the patterns and relationships in the data, allowing it to make accurate predictions or classifications without needing explicit programming.'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bm25_rag(question):\n",
    "    \"\"\"Complete BM25 RAG pipeline\"\"\"\n",
    "    print(f\"üìù BM25 RAG Pipeline\")\n",
    "    print(f\"Question: {question}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Step 1: BM25 keyword search\n",
    "    print(\"üîç Step 1: BM25 keyword search...\")\n",
    "    results = bm25_retriever.get_relevant_documents(question)\n",
    "    \n",
    "    # Step 2: Show retrieved documents\n",
    "    print(\"üìÑ Step 2: Retrieved documents:\")\n",
    "    for i, doc in enumerate(results):\n",
    "        print(f\"  {i+1}. {doc.page_content[:80]}...\")\n",
    "    \n",
    "    # Step 3: Generate answer\n",
    "    print(\"ü§ñ Step 3: Generating answer...\")\n",
    "    answer = generate_answer_from_bm25(question, results)\n",
    "    \n",
    "    print(f\"\\n‚úÖ BM25 Answer: {answer}\")\n",
    "    return answer\n",
    "\n",
    "# Test complete BM25 RAG\n",
    "bm25_rag(\"What is supervised learning?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7e1206",
   "metadata": {},
   "source": [
    "### Task : RRF - Contextual Rag\n",
    "\n",
    "#### read and implement "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b433adf",
   "metadata": {},
   "source": [
    "# Multilingual embeddings\n",
    "\n",
    "\n",
    "https://huggingface.co/intfloat/multilingual-e5-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f0120a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
