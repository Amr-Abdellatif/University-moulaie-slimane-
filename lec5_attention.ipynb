{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92a88d7f",
   "metadata": {},
   "source": [
    "Building Intuition for Attention Mechanisms - Step by Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb23fe3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bd6b25",
   "metadata": {},
   "source": [
    "Step 1: Understanding the Core Concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96fef449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([3, 4])\n",
      "Input embeddings:\n",
      "tensor([[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# Think of attention as asking: \"Which parts of the input should I focus on?\"\n",
    "# Let's start with a simple example - we have 3 words and want to understand \n",
    "# how much each word should \"attend\" to every other word\n",
    "\n",
    "# Our input: 3 words represented as vectors of size 4\n",
    "input_embeddings = torch.tensor([\n",
    "    [1.0, 0.0, 0.0, 0.0],  # word 1: \"cat\"\n",
    "    [0.0, 1.0, 0.0, 0.0],  # word 2: \"sat\" \n",
    "    [0.0, 0.0, 1.0, 0.0],  # word 3: \"mat\"\n",
    "], dtype=torch.float32)\n",
    "\n",
    "print(\"Input shape:\", input_embeddings.shape)  # [3, 4] - 3 words, 4 dimensions each\n",
    "print(\"Input embeddings:\")\n",
    "print(input_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40531fd0",
   "metadata": {},
   "source": [
    "Step 2: Computing Raw Attention Scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07353438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention scores (raw similarities):\n",
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]])\n",
      "Shape: torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "# The simplest attention: dot product between each pair of words\n",
    "# This tells us how \"similar\" or \"related\" each word is to every other word\n",
    "\n",
    "# For each word, compute its similarity with all words (including itself)\n",
    "attention_scores = torch.matmul(input_embeddings, input_embeddings.T)\n",
    "\n",
    "print(\"Attention scores (raw similarities):\")\n",
    "print(attention_scores)\n",
    "print(\"Shape:\", attention_scores.shape)  # [3, 3] - each word's score with every word\n",
    "\n",
    "# Let's interpret this:\n",
    "# attention_scores[0, 1] = how much word 0 (\"cat\") attends to word 1 (\"sat\")\n",
    "# attention_scores[1, 0] = how much word 1 (\"sat\") attends to word 0 (\"cat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014ca150",
   "metadata": {},
   "source": [
    "Step 3: Converting Scores to Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5961c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention weights (probabilities):\n",
      "tensor([[0.5761, 0.2119, 0.2119],\n",
      "        [0.2119, 0.5761, 0.2119],\n",
      "        [0.2119, 0.2119, 0.5761]])\n",
      "Row sums (should all be 1.0): tensor([1.0000, 1.0000, 1.0000])\n"
     ]
    }
   ],
   "source": [
    "# Raw scores aren't very interpretable. Let's convert them to probabilities\n",
    "# using softmax - now each row sums to 1.0\n",
    "\n",
    "attention_weights = F.softmax(attention_scores, dim=1)\n",
    "\n",
    "print(\"Attention weights (probabilities):\")\n",
    "print(attention_weights)\n",
    "print(\"Row sums (should all be 1.0):\", attention_weights.sum(dim=1))\n",
    "\n",
    "# Now we can interpret this as:\n",
    "# attention_weights[0, :] = how much word 0 attends to [word0, word1, word2]\n",
    "# These are probabilities, so word 0 pays 100% attention to itself here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93139e83",
   "metadata": {},
   "source": [
    "Step 4: Computing Attended Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e8a4916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attended output:\n",
      "tensor([[0.5761, 0.2119, 0.2119, 0.0000],\n",
      "        [0.2119, 0.5761, 0.2119, 0.0000],\n",
      "        [0.2119, 0.2119, 0.5761, 0.0000]])\n",
      "Shape: torch.Size([3, 4])\n"
     ]
    }
   ],
   "source": [
    "# Now we use these attention weights to create a weighted combination\n",
    "# of all the input embeddings for each word\n",
    "\n",
    "# For each word, sum up all embeddings weighted by attention\n",
    "attended_output = torch.matmul(attention_weights, input_embeddings)\n",
    "\n",
    "print(\"Attended output:\")\n",
    "print(attended_output)\n",
    "print(\"Shape:\", attended_output.shape)  # [3, 4] - same as input\n",
    "\n",
    "# Since attention weights are [1, 0, 0] for each word (attending only to themselves),\n",
    "# the output is identical to input in this simple case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec5f802",
   "metadata": {},
   "source": [
    "# Part2 - QKV "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85f8340",
   "metadata": {},
   "source": [
    "Step 5: Making it More Interesting - Learned Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5aebdb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query weight matrix:\n",
      "Parameter containing:\n",
      "tensor([[ 0.6621,  0.7188, -0.2029,  0.7955],\n",
      "        [-0.1897,  0.1748, -0.4216,  0.5086],\n",
      "        [ 0.7634, -0.6353,  0.7527,  0.1621],\n",
      "        [ 0.6398,  0.1173,  0.4176, -0.1223]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Let's make attention learnable by adding Query, Key, Value transformations\n",
    "# This is the foundation of transformer attention\n",
    "\n",
    "embed_dim = 4\n",
    "seq_len = 3\n",
    "\n",
    "# Simple linear transformations (no bias for clarity)\n",
    "W_q = nn.Linear(embed_dim, embed_dim, bias=False)  # Query transformation\n",
    "W_k = nn.Linear(embed_dim, embed_dim, bias=False)  # Key transformation  \n",
    "W_v = nn.Linear(embed_dim, embed_dim, bias=False)  # Value transformation\n",
    "\n",
    "# Initialize with small random weights\n",
    "torch.manual_seed(42)  # For reproducible results\n",
    "nn.init.xavier_uniform_(W_q.weight)\n",
    "nn.init.xavier_uniform_(W_k.weight) \n",
    "nn.init.xavier_uniform_(W_v.weight)\n",
    "\n",
    "print(\"Query weight matrix:\")\n",
    "print(W_q.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ab9357",
   "metadata": {},
   "source": [
    "Step 6: Computing Queries, Keys, and Values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbcbeba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queries shape: torch.Size([3, 4])\n",
      "Keys shape: torch.Size([3, 4])\n",
      "Values shape: torch.Size([3, 4])\n",
      "\n",
      "Queries (what each word asks about):\n",
      "tensor([[ 0.6621, -0.1897,  0.7634,  0.6398],\n",
      "        [ 0.7188,  0.1748, -0.6353,  0.1173],\n",
      "        [-0.2029, -0.4216,  0.7527,  0.4176]], grad_fn=<MmBackward0>)\n",
      "\n",
      "Keys (what each word represents):\n",
      "tensor([[ 0.6676, -0.3990, -0.6836,  0.0817],\n",
      "        [ 0.1280, -0.1016, -0.3992, -0.8554],\n",
      "        [-0.4043, -0.3517, -0.2445,  0.7821]], grad_fn=<MmBackward0>)\n",
      "\n",
      "Values (content each word offers):\n",
      "tensor([[ 0.6686,  0.1350,  0.2327,  0.5006],\n",
      "        [ 0.1441,  0.6997, -0.2348, -0.3786],\n",
      "        [-0.2812,  0.0947,  0.3645,  0.4999]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Transform our input embeddings into queries, keys, and values\n",
    "queries = W_q(input_embeddings)  # What each word is \"asking about\"\n",
    "keys = W_k(input_embeddings)     # What each word \"represents\" or \"offers\"\n",
    "values = W_v(input_embeddings)   # The actual \"content\" each word contributes\n",
    "\n",
    "print(\"Queries shape:\", queries.shape)\n",
    "print(\"Keys shape:\", keys.shape) \n",
    "print(\"Values shape:\", values.shape)\n",
    "\n",
    "print(\"\\nQueries (what each word asks about):\")\n",
    "print(queries)\n",
    "print(\"\\nKeys (what each word represents):\")\n",
    "print(keys)\n",
    "print(\"\\nValues (content each word offers):\")\n",
    "print(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01aa79c",
   "metadata": {},
   "source": [
    "Step 7: Scaled Dot-Product Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c3cce87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled attention scores:\n",
      "tensor([[ 0.0241, -0.3740,  0.0564],\n",
      "        [ 0.4270,  0.1138, -0.0525],\n",
      "        [-0.2238, -0.3204,  0.1864]], grad_fn=<DivBackward0>)\n",
      "\n",
      "Attention weights:\n",
      "tensor([[0.3698, 0.2483, 0.3819],\n",
      "        [0.4255, 0.3111, 0.2634],\n",
      "        [0.2928, 0.2659, 0.4413]], grad_fn=<SoftmaxBackward0>)\n",
      "Row sums: tensor([1., 1., 1.], grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# Now we compute attention using the learned Q, K, V\n",
    "# This is the core of transformer attention\n",
    "\n",
    "# Step 1: Compute attention scores (Q * K^T)\n",
    "attention_scores = torch.matmul(queries, keys.T)\n",
    "\n",
    "# Step 2: Scale by sqrt(d_k) to prevent softmax saturation\n",
    "d_k = keys.shape[-1]  # dimension of keys\n",
    "scaled_scores = attention_scores / np.sqrt(d_k)\n",
    "\n",
    "print(\"Scaled attention scores:\")\n",
    "print(scaled_scores)\n",
    "\n",
    "# Step 3: Apply softmax to get attention weights\n",
    "attention_weights = F.softmax(scaled_scores, dim=1)\n",
    "\n",
    "print(\"\\nAttention weights:\")\n",
    "print(attention_weights)\n",
    "print(\"Row sums:\", attention_weights.sum(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d09e0a",
   "metadata": {},
   "source": [
    "Step 8: Final Attended Output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc1fc17b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final attended output:\n",
      "tensor([[0.1756, 0.2598, 0.1669, 0.2820],\n",
      "        [0.2552, 0.3000, 0.1220, 0.2269],\n",
      "        [0.1100, 0.2673, 0.1666, 0.2666]], grad_fn=<MmBackward0>)\n",
      "Shape: torch.Size([3, 4])\n"
     ]
    }
   ],
   "source": [
    "# Apply attention weights to values (not the original embeddings!)\n",
    "attended_output = torch.matmul(attention_weights, values)\n",
    "\n",
    "print(\"Final attended output:\")\n",
    "print(attended_output)\n",
    "print(\"Shape:\", attended_output.shape)\n",
    "\n",
    "# This output is now a learned combination of values, weighted by attention\n",
    "# Each position contains information from all positions, weighted by relevance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be9e910",
   "metadata": {},
   "source": [
    "Step 9: Visualizing What Happened\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08b7f303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Distribution:\n",
      "==================================================\n",
      "Word 0 attends to:\n",
      "  Word 0: 0.370 (37.0%)\n",
      "  Word 1: 0.248 (24.8%)\n",
      "  Word 2: 0.382 (38.2%)\n",
      "\n",
      "Word 1 attends to:\n",
      "  Word 0: 0.426 (42.6%)\n",
      "  Word 1: 0.311 (31.1%)\n",
      "  Word 2: 0.263 (26.3%)\n",
      "\n",
      "Word 2 attends to:\n",
      "  Word 0: 0.293 (29.3%)\n",
      "  Word 1: 0.266 (26.6%)\n",
      "  Word 2: 0.441 (44.1%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's see how attention weights distribute focus\n",
    "print(\"Attention Distribution:\")\n",
    "print(\"=\" * 50)\n",
    "for i in range(seq_len):\n",
    "    print(f\"Word {i} attends to:\")\n",
    "    for j in range(seq_len):\n",
    "        weight = attention_weights[i, j].item()\n",
    "        print(f\"  Word {j}: {weight:.3f} ({weight*100:.1f}%)\")\n",
    "    print()\n",
    "\n",
    "# The beauty of attention: each word can focus on different parts of the sequence\n",
    "# These weights are learned during training to capture meaningful relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ef9536",
   "metadata": {},
   "source": [
    "# Part 3 - Mutli Headed Attention "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43425c2d",
   "metadata": {},
   "source": [
    "Step 10: Multi-Head Attention - The Concept\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a51e66de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 2 heads, each with 2 dimensions\n",
      "Total dimension: 4 = 4\n"
     ]
    }
   ],
   "source": [
    "# Multi-head attention is like having multiple \"attention experts\"\n",
    "# Each head can focus on different types of relationships:\n",
    "# - Head 1 might focus on syntactic relationships\n",
    "# - Head 2 might focus on semantic relationships  \n",
    "# - Head 3 might focus on positional relationships\n",
    "\n",
    "# Think of it as asking multiple questions about the same input:\n",
    "# \"What's grammatically related?\" \"What's semantically related?\" \"What's nearby?\"\n",
    "\n",
    "num_heads = 2  # We'll use 2 heads for simplicity\n",
    "embed_dim = 4  # Keep our embedding dimension small\n",
    "head_dim = embed_dim // num_heads  # Each head gets 2 dimensions\n",
    "\n",
    "print(f\"Using {num_heads} heads, each with {head_dim} dimensions\")\n",
    "print(f\"Total dimension: {num_heads * head_dim} = {embed_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1aacf7",
   "metadata": {},
   "source": [
    "Step 12: Creating Multiple Q, K, V Matrices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a394850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-head Q weight shape: torch.Size([4, 4])\n",
      "Multi-head K weight shape: torch.Size([4, 4])\n",
      "Multi-head V weight shape: torch.Size([4, 4])\n"
     ]
    }
   ],
   "source": [
    "# Instead of one W_q, W_k, W_v, we need separate matrices for each head\n",
    "# We'll pack them all into larger matrices and split later\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Create combined matrices for all heads\n",
    "# Each head gets head_dim dimensions, so total is num_heads * head_dim\n",
    "W_q_multi = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "W_k_multi = nn.Linear(embed_dim, embed_dim, bias=False)  \n",
    "W_v_multi = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "\n",
    "# Initialize\n",
    "nn.init.xavier_uniform_(W_q_multi.weight)\n",
    "nn.init.xavier_uniform_(W_k_multi.weight)\n",
    "nn.init.xavier_uniform_(W_v_multi.weight)\n",
    "\n",
    "print(\"Multi-head Q weight shape:\", W_q_multi.weight.shape)\n",
    "print(\"Multi-head K weight shape:\", W_k_multi.weight.shape)\n",
    "print(\"Multi-head V weight shape:\", W_v_multi.weight.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5994e26c",
   "metadata": {},
   "source": [
    "Step 13: Splitting Into Multiple Heads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ce7f453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Q shape: torch.Size([1, 3, 4])\n",
      "Q heads shape: torch.Size([1, 3, 2, 2])\n",
      "Q heads after transpose: torch.Size([1, 2, 3, 2])\n",
      "Now we have 2 separate attention heads!\n"
     ]
    }
   ],
   "source": [
    "# Transform input and split into multiple heads\n",
    "seq_len = 3\n",
    "batch_size = 1\n",
    "\n",
    "# Add batch dimension for easier manipulation\n",
    "input_batch = input_embeddings.unsqueeze(0)  # [1, 3, 4]\n",
    "\n",
    "# Get Q, K, V for all heads combined\n",
    "Q_all = W_q_multi(input_batch)  # [1, 3, 4]\n",
    "K_all = W_k_multi(input_batch)  # [1, 3, 4]\n",
    "V_all = W_v_multi(input_batch)  # [1, 3, 4]\n",
    "\n",
    "print(\"Combined Q shape:\", Q_all.shape)\n",
    "\n",
    "# Split into multiple heads\n",
    "# Reshape: [batch, seq_len, embed_dim] -> [batch, seq_len, num_heads, head_dim]\n",
    "Q_heads = Q_all.view(batch_size, seq_len, num_heads, head_dim)\n",
    "K_heads = K_all.view(batch_size, seq_len, num_heads, head_dim)\n",
    "V_heads = V_all.view(batch_size, seq_len, num_heads, head_dim)\n",
    "\n",
    "print(\"Q heads shape:\", Q_heads.shape)  # [1, 3, 2, 2]\n",
    "\n",
    "# Transpose to get: [batch, num_heads, seq_len, head_dim]\n",
    "Q_heads = Q_heads.transpose(1, 2)  # [1, 2, 3, 2]\n",
    "K_heads = K_heads.transpose(1, 2)  # [1, 2, 3, 2]\n",
    "V_heads = V_heads.transpose(1, 2)  # [1, 2, 3, 2]\n",
    "\n",
    "print(\"Q heads after transpose:\", Q_heads.shape)\n",
    "print(\"Now we have\", num_heads, \"separate attention heads!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b846d1",
   "metadata": {},
   "source": [
    "Step 14: Computing Attention for Each Head\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a363847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Head 0 ---\n",
      "Head 0 Q shape: torch.Size([3, 2])\n",
      "Head 0 Q values:\n",
      "tensor([[ 0.4398, -0.6643],\n",
      "        [-0.5278,  0.7106],\n",
      "        [-0.8573,  0.2494]], grad_fn=<SelectBackward0>)\n",
      "Head 0 attention weights:\n",
      "tensor([[0.4005, 0.3486, 0.2509],\n",
      "        [0.2631, 0.3108, 0.4261],\n",
      "        [0.3063, 0.4013, 0.2925]], grad_fn=<SoftmaxBackward0>)\n",
      "Head 0 output shape: torch.Size([3, 2])\n",
      "\n",
      "--- Head 1 ---\n",
      "Head 1 Q shape: torch.Size([3, 2])\n",
      "Head 1 Q values:\n",
      "tensor([[ 0.2739,  0.0545],\n",
      "        [-0.0151, -0.5911],\n",
      "        [ 0.6778,  0.2670]], grad_fn=<SelectBackward0>)\n",
      "Head 1 attention weights:\n",
      "tensor([[0.3978, 0.3055, 0.2967],\n",
      "        [0.4185, 0.2880, 0.2934],\n",
      "        [0.4776, 0.2712, 0.2512]], grad_fn=<SoftmaxBackward0>)\n",
      "Head 1 output shape: torch.Size([3, 2])\n",
      "\n",
      "We now have 2 head outputs, each with shape torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "# Now we compute attention separately for each head\n",
    "# Each head will have its own attention pattern\n",
    "\n",
    "def compute_head_attention(Q, K, V):\n",
    "    \"\"\"Compute attention for a single head\"\"\"\n",
    "    d_k = K.shape[-1]\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(d_k)\n",
    "    weights = F.softmax(scores, dim=-1)\n",
    "    output = torch.matmul(weights, V)\n",
    "    return output, weights\n",
    "\n",
    "# Process each head separately\n",
    "head_outputs = []\n",
    "head_weights = []\n",
    "\n",
    "for h in range(num_heads):\n",
    "    print(f\"\\n--- Head {h} ---\")\n",
    "    \n",
    "    # Extract this head's Q, K, V\n",
    "    Q_h = Q_heads[0, h]  # [3, 2] - Remove batch dim for clarity\n",
    "    K_h = K_heads[0, h]  # [3, 2]\n",
    "    V_h = V_heads[0, h]  # [3, 2]\n",
    "    \n",
    "    print(f\"Head {h} Q shape:\", Q_h.shape)\n",
    "    print(f\"Head {h} Q values:\")\n",
    "    print(Q_h)\n",
    "    \n",
    "    # Compute attention for this head\n",
    "    output_h, weights_h = compute_head_attention(Q_h, K_h, V_h)\n",
    "    \n",
    "    print(f\"Head {h} attention weights:\")\n",
    "    print(weights_h)\n",
    "    print(f\"Head {h} output shape:\", output_h.shape)\n",
    "    \n",
    "    head_outputs.append(output_h)\n",
    "    head_weights.append(weights_h)\n",
    "\n",
    "print(f\"\\nWe now have {len(head_outputs)} head outputs, each with shape {head_outputs[0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a820f399",
   "metadata": {},
   "source": [
    "Step 15: Concatenating Head Outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd5554a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated output shape: torch.Size([3, 4])\n",
      "Concatenated output:\n",
      "tensor([[-0.2697,  0.2631, -0.0389,  0.1989],\n",
      "        [-0.1054,  0.2855, -0.0531,  0.2013],\n",
      "        [-0.2395,  0.2234, -0.0911,  0.2485]], grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Combine all head outputs back together\n",
    "# Each head produced [3, 2] output, we want [3, 4] final output\n",
    "\n",
    "# Stack along the last dimension and reshape\n",
    "concatenated = torch.cat(head_outputs, dim=-1)  # [3, 4]\n",
    "\n",
    "print(\"Concatenated output shape:\", concatenated.shape)\n",
    "print(\"Concatenated output:\")\n",
    "print(concatenated)\n",
    "\n",
    "# This concatenated output contains information from all heads\n",
    "# Each head contributed its own \"perspective\" on the input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb866f8",
   "metadata": {},
   "source": [
    "Step 16: Final Output Projection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ace98147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final multi-head attention output:\n",
      "tensor([[ 0.1767,  0.2147, -0.3701, -0.1822],\n",
      "        [ 0.3064,  0.1947, -0.2692, -0.0808],\n",
      "        [ 0.2182,  0.2493, -0.3531, -0.1955]], grad_fn=<MmBackward0>)\n",
      "Shape: torch.Size([3, 4])\n"
     ]
    }
   ],
   "source": [
    "# In real transformers, there's usually a final linear projection\n",
    "# This allows the model to mix information from different heads\n",
    "\n",
    "W_o = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "torch.manual_seed(42)\n",
    "nn.init.xavier_uniform_(W_o.weight)\n",
    "\n",
    "# Apply output projection\n",
    "multi_head_output = W_o(concatenated)\n",
    "\n",
    "print(\"Final multi-head attention output:\")\n",
    "print(multi_head_output)\n",
    "print(\"Shape:\", multi_head_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86b9932",
   "metadata": {},
   "source": [
    "Step 17: Complete Multi-Head Attention Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e8b6fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-head attention output shape: torch.Size([1, 3, 4])\n",
      "Attention weights shape: torch.Size([1, 2, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        # Combined linear layers for all heads\n",
    "        self.W_q = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.W_k = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.W_v = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        \n",
    "        # Output projection\n",
    "        self.W_o = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, embed_dim = x.shape\n",
    "        \n",
    "        # Get Q, K, V for all heads\n",
    "        Q = self.W_q(x)  # [batch, seq_len, embed_dim]\n",
    "        K = self.W_k(x)\n",
    "        V = self.W_v(x)\n",
    "        \n",
    "        # Reshape and transpose for multi-head processing\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Scaled dot-product attention for all heads in parallel\n",
    "        d_k = self.head_dim\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(d_k)\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        attended = torch.matmul(weights, V)\n",
    "        \n",
    "        # Concatenate heads\n",
    "        attended = attended.transpose(1, 2).contiguous().view(\n",
    "            batch_size, seq_len, self.embed_dim\n",
    "        )\n",
    "        \n",
    "        # Final projection\n",
    "        output = self.W_o(attended)\n",
    "        \n",
    "        return output, weights\n",
    "\n",
    "# Test our implementation\n",
    "mha = MultiHeadAttention(embed_dim=4, num_heads=2)\n",
    "test_input = input_embeddings.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "output, attention_weights = mha(test_input)\n",
    "print(\"Multi-head attention output shape:\", output.shape)\n",
    "print(\"Attention weights shape:\", attention_weights.shape)  # [batch, heads, seq, seq]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b932814",
   "metadata": {},
   "source": [
    "Step 18: Visualizing Multi-Head Attention Patterns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "118ac9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-Head Attention Patterns:\n",
      "============================================================\n",
      "\n",
      "Head 0 attention pattern:\n",
      "  Word 0 -> Word0:0.365 Word1:0.319 Word2:0.316 \n",
      "  Word 1 -> Word0:0.340 Word1:0.330 Word2:0.330 \n",
      "  Word 2 -> Word0:0.322 Word1:0.323 Word2:0.354 \n",
      "  Head 0 focuses most on positions: [0, 0, 2]\n",
      "\n",
      "Head 1 attention pattern:\n",
      "  Word 0 -> Word0:0.329 Word1:0.349 Word2:0.322 \n",
      "  Word 1 -> Word0:0.309 Word1:0.386 Word2:0.305 \n",
      "  Word 2 -> Word0:0.349 Word1:0.305 Word2:0.346 \n",
      "  Head 1 focuses most on positions: [1, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "# Let's see what each head learned to focus on\n",
    "print(\"Multi-Head Attention Patterns:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for head in range(num_heads):\n",
    "        print(f\"\\nHead {head} attention pattern:\")\n",
    "        head_weights = attention_weights[0, head]  # Remove batch dim\n",
    "        \n",
    "        for i in range(seq_len):\n",
    "            print(f\"  Word {i} -> \", end=\"\")\n",
    "            for j in range(seq_len):\n",
    "                weight = head_weights[i, j].item()\n",
    "                print(f\"Word{j}:{weight:.3f} \", end=\"\")\n",
    "            print()\n",
    "        \n",
    "        # Show which positions this head focuses on most\n",
    "        max_attention = head_weights.max(dim=1)\n",
    "        print(f\"  Head {head} focuses most on positions:\", max_attention.indices.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442ac25f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
