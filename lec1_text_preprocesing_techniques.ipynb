{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be0db01f",
   "metadata": {},
   "source": [
    "# Text Preprocessing Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8563d05b",
   "metadata": {},
   "source": [
    "1. Lowercase Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dbb6115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world! this is an example.\n"
     ]
    }
   ],
   "source": [
    "def lowercase_text(text):\n",
    "    return text.lower()\n",
    "\n",
    "text = \"Hello World! This is an Example.\"\n",
    "lowercase_text = lowercase_text(text)\n",
    "print(lowercase_text)  # hello world! this is an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff375ac8",
   "metadata": {},
   "source": [
    "2. Stop Word Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964e3ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.1\n"
     ]
    }
   ],
   "source": [
    "# !pip install nltk -q --user\n",
    "import nltk\n",
    "print(nltk.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3a50dbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Amr osama\n",
      "[nltk_data]     abdellatif\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download NLTK stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae7dca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'world', 'example']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "def remove_stopwords(tokens):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [token for token in tokens if token.lower() not in stop_words]\n",
    "tokens = ['hello', 'world', 'this', 'is','The', 'an', 'example']\n",
    "filtered_tokens = remove_stopwords(tokens)\n",
    "print(filtered_tokens)  # ['hello', 'world', 'example']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072a9724",
   "metadata": {},
   "source": [
    "3. Punctuation Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c0b2ae",
   "metadata": {},
   "source": [
    "4. Regular Expressions (Regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7b463c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world This is an example with punctuation\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "text = \"Hello, world! This is an example: with punctuation.\"\n",
    "clean_text = remove_punctuation(text)\n",
    "print(clean_text)  # \"Hello world This is an example with punctuation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68e49846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python\n"
     ]
    }
   ],
   "source": [
    "# Match Simple Text\n",
    "import re\n",
    "\n",
    "text = \"Python is fun\"\n",
    "match = re.search(\"Python\", text)\n",
    "print(match.group())  # Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa5dd6e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python\n",
      "amazing\n"
     ]
    }
   ],
   "source": [
    "# Match Beginning and End\n",
    "\n",
    "# ^ matches start of string, $ matches end\n",
    "text = \"Python is amazing\"\n",
    "start_match = re.search(\"^Python\", text)\n",
    "# in case of error means that re.search(\"^Python\", text) did not find a match,\n",
    "#  so it returned None, and you're trying to call .group() on None.\n",
    "# print(start_match)\n",
    "print(start_match.group())  # Python\n",
    "\n",
    "end_match = re.search(\"amazing$\", text)\n",
    "print(end_match.group())  # amazing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2052a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3', '3', '5']\n",
      "['3', '35']\n"
     ]
    }
   ],
   "source": [
    "# Match Digits\n",
    "\n",
    "text = \"I have 3 apples and 35 oranges\"\n",
    "digits = re.findall(r\"\\d\", text)  # r prefix creates a raw string\n",
    "print(digits)  # ['3', '5']\n",
    "\n",
    "text = \"I have 3 apples and 35 oranges\"\n",
    "# \\d+ matches one or more digits\n",
    "numbers = re.findall(r\"\\d+\", text)\n",
    "print(numbers)  # ['3', '5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fea881a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['user_123', 'has', 'logged', 'in']\n"
     ]
    }
   ],
   "source": [
    "# Match Word Characters\n",
    "\n",
    "text = \"user_123 has logged in\"\n",
    "# \\w matches alphanumeric + underscore\n",
    "word_chars = re.findall(r\"\\w+\", text)\n",
    "print(word_chars)  # ['user_123', 'has', 'logged', 'in']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8a8524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['color', 'colour']\n"
     ]
    }
   ],
   "source": [
    "# matching zero or more \n",
    "\n",
    "text = \"color colour colouur\"\n",
    "pattern = re.findall(r\"colou?r\", text)  # ? means 0 or 1 of previous character\n",
    "print(pattern)  # ['color', 'colour']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa36dce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loooove']\n"
     ]
    }
   ],
   "source": [
    "# Match One or More\n",
    "\n",
    "text = \"I loooove Python\"\n",
    "pattern = re.findall(r\"lo+ve\", text)  # + means 1 or more of previous character\n",
    "print(pattern)  # ['loooove']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b81a9f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['555-1234', '678-5678']\n"
     ]
    }
   ],
   "source": [
    "# Match Exact Number\n",
    "\n",
    "\n",
    "text = \"Phone numbers: 555-1234 and 555678-5678\"\n",
    "pattern = re.findall(r\"\\d{3}-\\d{4}\", text)  # {n} means exactly n occurrences\n",
    "print(pattern)  # ['555-1234', '555-5678']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a484cf2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cat', 'rat']\n"
     ]
    }
   ],
   "source": [
    "# Match Any of Several Characters\n",
    "\n",
    "text = \"The cat and the rat sat on the mat\"\n",
    "pattern = re.findall(r\"[cr]at\", text)  # matches 'cat' or 'rat'\n",
    "print(pattern)  # ['cat', 'rat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bfb0bea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', 'c']\n",
      "['D', 'E']\n",
      "['a', '1', 'b', '2', 'c', '3', 'D', '4', 'E', '5']\n"
     ]
    }
   ],
   "source": [
    "# Match Range of Characters\n",
    "\n",
    "text = \"a1b2c3D4E5\"\n",
    "letters = re.findall(r\"[a-z]\", text)  # lowercase letters\n",
    "print(letters)  # ['a', 'b', 'c']\n",
    "\n",
    "uppercase = re.findall(r\"[A-Z]\", text)  # uppercase letters\n",
    "print(uppercase)  # ['D', 'E']\n",
    "\n",
    "alphanumeric = re.findall(r\"[a-zA-Z0-9]\", text)  # all alphanumeric\n",
    "print(alphanumeric)  # ['a', '1', 'b', '2', 'c', '3', 'D', '4', 'E', '5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd32b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user@example.com is valid\n",
      "invalid@email is invalid\n",
      "name.last@domain.co.uk is valid\n"
     ]
    }
   ],
   "source": [
    "# Email Validation\n",
    "# . (unescaped dot) = matches ANY character except newline (wildcard)\n",
    "# \\. (escaped dot) = matches a literal dot character\n",
    "# {2,} at least 2 chars\n",
    "\n",
    "emails = [\"user@example.com\", \"invalid@email\", \"name.last@domain.co.uk\"]\n",
    "pattern = r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\"\n",
    "\n",
    "for email in emails:\n",
    "    if re.match(pattern, email):\n",
    "        print(f\"{email} is valid\")\n",
    "    else:\n",
    "        print(f\"{email} is invalid\")\n",
    "# user@example.com is valid\n",
    "# invalid@email is invalid\n",
    "# name.last@domain.co.uk is valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8e73f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are  apples and  oranges.\n",
      "There are NUM apples and NUM oranges.\n"
     ]
    }
   ],
   "source": [
    "# Number Removal/Normalization\n",
    "# /d is for digits\n",
    "import re\n",
    "\n",
    "def remove_numbers(text):\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "def replace_numbers(text, replacement='NUM'):\n",
    "    return re.sub(r'\\d+', replacement, text)\n",
    "\n",
    "text = \"There are 123 apples and 456 oranges.\"\n",
    "text_no_numbers = remove_numbers(text)\n",
    "text_normalized = replace_numbers(text)\n",
    "\n",
    "print(text_no_numbers)  # \"There are  apples and  oranges.\"\n",
    "print(text_normalized)  # \"There are NUM apples and NUM oranges.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73a85c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special characters unicode like  should be removed\n"
     ]
    }
   ],
   "source": [
    "# Noise Removal\n",
    "import re\n",
    "\n",
    "def remove_noise(text):\n",
    "    # Remove special characters and symbols\n",
    "    # [^] match that is not a word or white space (spaces, tabs, newlines) char\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Remove ASCII/Unicode characters\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "text = \"Special @#! characters & unicode like 你好 should    be removed.\"\n",
    "clean_text = remove_noise(text)\n",
    "print(clean_text)  # \"Special characters  unicode like  should be removed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade3950e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contact us at [EMAIL] or visit [URL] or call [PHONE]\n"
     ]
    }
   ],
   "source": [
    "# Text Normalization with REGEX\n",
    "\n",
    "import re\n",
    "\n",
    "def normalize_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Replace URLs\n",
    "    # \\S means \"non-whitespace character\" (the opposite of \\s)\n",
    "    # \\s = whitespace (spaces, tabs, newlines)\n",
    "    # \\S = any character that is NOT whitespace\n",
    "\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '[URL]', text)\n",
    "    \n",
    "    # Replace emails\n",
    "    text = re.sub(r'\\S+@\\S+', '[EMAIL]', text)\n",
    "    \n",
    "    # Replace phone numbers\n",
    "    text = re.sub(r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}', '[PHONE]', text)\n",
    "    \n",
    "    # Replace multiple whitespaces with single\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Replace elongated words (e.g., \"hellooooo\" -> \"hello\")\n",
    "    text = re.sub(r'(.)\\1{2,}', r'\\1', text)\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "text = \"Contact us at example@gmail.com or visit https://.example.com or call 123-456-7890\"\n",
    "normalized_text = normalize_text(text)\n",
    "print(normalized_text)  # \"contact us at [EMAIL] or visit [URL] or call [PHONE]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62580390",
   "metadata": {},
   "source": [
    "5. Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92ca099",
   "metadata": {},
   "source": [
    "Tokenization is the process of splitting text into smaller pieces, called tokens.\n",
    "These tokens can be:\n",
    "\n",
    "Words → Word-level tokenization\n",
    "\n",
    "Characters → Character-level tokenization\n",
    "\n",
    "Subwords → Subword-level tokenization (used in models like BERT, GPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bce6f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'world', '.', 'How', 'are', 'you', 'today', '?']\n",
      "['Hello world.', 'How are you today?']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Amr osama\n",
      "[nltk_data]     abdellatif\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Using NLTK\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "def tokenize_text(text):\n",
    "    # Word tokenization\n",
    "    word_tokens = nltk.word_tokenize(text)\n",
    "    # Sentence tokenization\n",
    "    sentence_tokens = nltk.sent_tokenize(text)\n",
    "    return word_tokens, sentence_tokens\n",
    "\n",
    "text = \"Hello world. How are you today?\"\n",
    "word_tokens, sentence_tokens = tokenize_text(text)\n",
    "print(word_tokens)  # ['Hello', 'world', '.', 'How', 'are', 'you', 'today', '?']\n",
    "print(sentence_tokens)  # ['Hello world.', 'How are you today?']\n",
    "\n",
    "\n",
    "# we can look at letters as well"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef12ab27",
   "metadata": {},
   "source": [
    "Bert tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13a0294e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'world', '.', 'how', 'are', 'you', 'today', '?']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokens = tokenizer.tokenize(\"Hello world. How are you today?\")\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc8ab40b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Ġworld', '.', 'ĠHow', 'Ġare', 'Ġyou', 'Ġtoday', '?']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokens = tokenizer.tokenize(\"Hello world. How are you today?\")\n",
    "print(tokens)\n",
    "\n",
    "# The Ġ symbol represents a space before the word (tokenized using byte-level BPE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efe6c0f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['un', 'bel', 'iev', 'ability']\n",
      "['ban', 'ana', 'Ġ']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokens = tokenizer.tokenize(\"unbelievability\")\n",
    "print(tokens)\n",
    "tokens = tokenizer.tokenize(\"banana \")\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe5bcfd",
   "metadata": {},
   "source": [
    "6. Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "930bbb75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter: ['run', 'run', 'ran', 'easili', 'fairli']\n",
      "Lancaster: ['run', 'run', 'ran', 'easy', 'fair']\n",
      "Snowball: ['run', 'run', 'ran', 'easili', 'fair']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer\n",
    "\n",
    "def stem_words(tokens):\n",
    "    porter = PorterStemmer()\n",
    "    lancaster = LancasterStemmer()\n",
    "    snowball = SnowballStemmer('english')\n",
    "    \n",
    "    porter_stems = [porter.stem(token) for token in tokens]\n",
    "    lancaster_stems = [lancaster.stem(token) for token in tokens]\n",
    "    snowball_stems = [snowball.stem(token) for token in tokens]\n",
    "    \n",
    "    return porter_stems, lancaster_stems, snowball_stems\n",
    "\n",
    "tokens = ['running', 'runs', 'ran', 'easily', 'fairly']\n",
    "porter_stems, lancaster_stems, snowball_stems = stem_words(tokens)\n",
    "print(f\"Porter: {porter_stems}\")    # ['run', 'run', 'ran', 'easili', 'fairli']\n",
    "print(f\"Lancaster: {lancaster_stems}\")  # ['run', 'run', 'ran', 'easy', 'fair']\n",
    "print(f\"Snowball: {snowball_stems}\")    # ['run', 'run', 'ran', 'easili', 'fair']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b86d4ed",
   "metadata": {},
   "source": [
    "7. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd16445e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Amr osama\n",
      "[nltk_data]     abdellatif\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['running', 'run', 'ran', 'better', 'mouse']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def lemmatize_words(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "tokens = ['running', 'runs', 'ran', 'better', 'mice']\n",
    "lemmatized_tokens = lemmatize_words(tokens)\n",
    "print(lemmatized_tokens)  # ['running', 'run', 'ran', 'better', 'mouse']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561f4cf1",
   "metadata": {},
   "source": [
    "8. Spell Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f2adb167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['help', 'world', 'example']\n"
     ]
    }
   ],
   "source": [
    "# pip install pyspellchecker spellchecker\n",
    "\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "def correct_spelling(tokens):\n",
    "    spell = SpellChecker()\n",
    "    corrected = [spell.correction(token) for token in tokens]\n",
    "    return corrected\n",
    "\n",
    "tokens = ['helo', 'wrld', 'example']\n",
    "corrected_tokens = correct_spelling(tokens)\n",
    "print(corrected_tokens)  # ['hello', 'world', 'example']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9eaff4a",
   "metadata": {},
   "source": [
    "9. Text Normalization with TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2d4626d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to C:\\Users\\Amr osama\n",
      "[nltk_data]     abdellatif\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\brown.zip.\n",
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\Amr osama\n",
      "[nltk_data]     abdellatif\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Amr osama\n",
      "[nltk_data]     abdellatif\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\Amr osama\n",
      "[nltk_data]     abdellatif\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package conll2000 to C:\\Users\\Amr osama\n",
      "[nltk_data]     abdellatif\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\conll2000.zip.\n",
      "[nltk_data] Downloading package movie_reviews to C:\\Users\\Amr osama\n",
      "[nltk_data]     abdellatif\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\movie_reviews.zip.\n"
     ]
    }
   ],
   "source": [
    "# !pip install textblob -q --user\n",
    "!python -m textblob.download_corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fb885728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected: good feeling\n",
      "Sentiment: Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "Noun phrases: ['guuod feeling']\n"
     ]
    }
   ],
   "source": [
    "'''TextBlob is a Python library that provides a simple API\n",
    "for common natural language processing (NLP) tasks.\n",
    "It's built on top of NLTK (Natural Language Toolkit) and\n",
    "Pattern, making it easier to perform text analysis without diving\n",
    "into the complexities of those underlying libraries.\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "def normalize_with_textblob(text):\n",
    "    blob = TextBlob(text)\n",
    "    \n",
    "    # Correct spelling\n",
    "    corrected = blob.correct()\n",
    "    \n",
    "    # Get sentiment\n",
    "    sentiment = blob.sentiment\n",
    "    \n",
    "    # Get noun phrases\n",
    "    noun_phrases = blob.noun_phrases\n",
    "    \n",
    "    return str(corrected), sentiment, noun_phrases\n",
    "\n",
    "text = \"The quik brown fox jumpd over the lazzy dog.\"\n",
    "text = \"guuod feeling\"\n",
    "corrected, sentiment, noun_phrases = normalize_with_textblob(text)\n",
    "\n",
    "print(f\"Corrected: {corrected}\")\n",
    "print(f\"Sentiment: {sentiment}\")\n",
    "print(f\"Noun phrases: {noun_phrases}\")\n",
    "\n",
    "# Corrected: The quick brown fox jumped over the lazy dog.\n",
    "# Sentiment: Sentiment(polarity=0.0, subjectivity=0.0)\n",
    "# Noun phrases: ['brown fox jumpd', 'lazzy dog']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f09875",
   "metadata": {},
   "source": [
    "10. Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "caa29d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Apple', 'ORG'), ('U.K.', 'GPE'), ('$1 billion', 'MONEY')]\n"
     ]
    }
   ],
   "source": [
    "# pip intall spacy\n",
    "# python -m spacy download en_core_web_sm\n",
    "import spacy\n",
    "\n",
    "def extract_entities(text):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    return entities\n",
    "\n",
    "text = \"Apple is looking at buying U.K. startup for $1 billion\"\n",
    "entities = extract_entities(text)\n",
    "print(entities)  # [('Apple', 'ORG'), ('U.K.', 'GPE'), ('$1 billion', 'MONEY')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a78bd2a",
   "metadata": {},
   "source": [
    "11. Text Cleaning (HTML/XML tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1b466396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is sample HTML text?.\n",
      "This is sample HTML text?.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def clean_html(html_text):\n",
    "    # Using BeautifulSoup\n",
    "    soup = BeautifulSoup(html_text, \"html.parser\")\n",
    "    clean_text = soup.get_text(separator=\" \", strip=True)\n",
    "    return clean_text\n",
    "\n",
    "def clean_html_regex(html_text):\n",
    "    # Using regex\n",
    "    clean_text = re.sub(r'<.*?>', '', html_text)\n",
    "    return clean_text\n",
    "\n",
    "html = \"<div><p>This is <b>sample</b> HTML text?.</p></div>\"\n",
    "clean_bs = clean_html(html)\n",
    "clean_re = clean_html_regex(html)\n",
    "\n",
    "print(clean_bs)  # \"This is sample HTML text.\"\n",
    "print(clean_re)  # \"This is sample HTML text.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24af3bd",
   "metadata": {},
   "source": [
    "12. Contractions Expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1559017f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I cannot do this and I will not try it.\n"
     ]
    }
   ],
   "source": [
    "# pip install contractions\n",
    "import contractions\n",
    "\n",
    "def expand_contractions(text):\n",
    "    expanded_text = contractions.fix(text)\n",
    "    return expanded_text\n",
    "\n",
    "text = \"I can't do this and I won't try it.\"\n",
    "expanded = expand_contractions(text)\n",
    "print(expanded)  # \"I cannot do this and I will not try it.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb02608",
   "metadata": {},
   "source": [
    "18. Language Detection and Translation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2e105e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~ich (C:\\Users\\Amr osama abdellatif\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ich (C:\\Users\\Amr osama abdellatif\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "chromadb 0.5.5 requires httpx>=0.27.0, but you have httpx 0.13.3 which is incompatible.\n",
      "cohere 5.9.1 requires httpx>=0.21.2, but you have httpx 0.13.3 which is incompatible.\n",
      "gotrue 2.9.3 requires httpx[http2]<0.28,>=0.26, but you have httpx 0.13.3 which is incompatible.\n",
      "gradio 5.4.0 requires httpx>=0.24.1, but you have httpx 0.13.3 which is incompatible.\n",
      "gradio 5.4.0 requires tomlkit==0.12.0, but you have tomlkit 0.12.5 which is incompatible.\n",
      "gradio 5.4.0 requires typer<1.0,>=0.12; sys_platform != \"emscripten\", but you have typer 0.9.4 which is incompatible.\n",
      "gradio-client 1.4.2 requires httpx>=0.24.1, but you have httpx 0.13.3 which is incompatible.\n",
      "langchain-cohere 0.2.0 requires langchain-core<0.3,>=0.2.24, but you have langchain-core 0.3.66 which is incompatible.\n",
      "langgraph-sdk 0.1.70 requires httpx>=0.25.2, but you have httpx 0.13.3 which is incompatible.\n",
      "langserve 0.2.3 requires httpx>=0.23.0, but you have httpx 0.13.3 which is incompatible.\n",
      "langserve 0.2.3 requires langchain-core<0.3,>=0.1, but you have langchain-core 0.3.66 which is incompatible.\n",
      "langsmith 0.4.2 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\n",
      "ollama 0.5.1 requires httpx>=0.27, but you have httpx 0.13.3 which is incompatible.\n",
      "openai 1.53.0 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\n",
      "postgrest 0.17.2 requires httpx[http2]<0.28,>=0.26, but you have httpx 0.13.3 which is incompatible.\n",
      "roboflow 1.1.66 requires idna==3.7, but you have idna 2.10 which is incompatible.\n",
      "storage3 0.8.2 requires httpx[http2]<0.28,>=0.26, but you have httpx 0.13.3 which is incompatible.\n",
      "streamlit 1.41.1 requires rich<14,>=10.14.0, but you have rich 14.0.0 which is incompatible.\n",
      "supabase 2.9.1 requires httpx<0.28,>=0.26, but you have httpx 0.13.3 which is incompatible.\n",
      "supafunc 0.6.2 requires httpx[http2]<0.28,>=0.26, but you have httpx 0.13.3 which is incompatible.\n",
      "swarms 5.8.7 requires langchain-community==0.0.29, but you have langchain-community 0.3.26 which is incompatible.\n",
      "swarms 5.8.7 requires Pillow==10.4.0, but you have pillow 11.1.0 which is incompatible.\n",
      "swarms 5.8.7 requires pydantic==2.8.2, but you have pydantic 2.11.7 which is incompatible.\n",
      "swarms-cloud 0.4.4 requires fastapi==0.112.2, but you have fastapi 0.115.4 which is incompatible.\n",
      "swarms-cloud 0.4.4 requires sse-starlette==2.0.0, but you have sse-starlette 1.8.2 which is incompatible.\n",
      "swarm-models 0.1.1 requires langchain-community==0.0.29, but you have langchain-community 0.3.26 which is incompatible.\n",
      "together 1.3.3 requires pillow<11.0.0,>=10.3.0, but you have pillow 11.1.0 which is incompatible.\n",
      "together 1.3.3 requires rich<14.0.0,>=13.8.1, but you have rich 14.0.0 which is incompatible.\n",
      "unstructured-client 0.25.1 requires httpx>=0.27.0, but you have httpx 0.13.3 which is incompatible.\n",
      "unstructured-client 0.25.1 requires idna>=3.4, but you have idna 2.10 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "WARNING: Ignoring invalid distribution ~ich (C:\\Users\\Amr osama abdellatif\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~ich (C:\\Users\\Amr osama abdellatif\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install googletrans==4.0.0rc1 -q --user\n",
    "!pip install langdetect -q --user\n",
    "# restart kernel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3e8cbfa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language: fr\n",
      "Translation: Hello world\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langdetect import detect\n",
    "from googletrans import Translator\n",
    "\n",
    "def detect_and_translate(text, target_lang='en'):\n",
    "    # Detect language\n",
    "    source_lang = detect(text)\n",
    "    \n",
    "    # Translate text\n",
    "    translator = Translator()\n",
    "    translation = translator.translate(text, src=source_lang, dest=target_lang)\n",
    "    \n",
    "    return source_lang, translation.text\n",
    "\n",
    "text = \"Bonjour le monde\"\n",
    "source, translation = detect_and_translate(text)\n",
    "print(f\"Detected language: {source}\")\n",
    "print(f\"Translation: {translation}\")\n",
    "\n",
    "# Detected language: fr\n",
    "# Translation: Hello world"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d764425",
   "metadata": {},
   "source": [
    "19. Custom Vocabulary Creation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8742d718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: {'this': 4, 'is': 4, 'the': 4, 'document': 4, '.': 3, 'first': 2}\n",
      "Token to ID mapping: {'this': 0, 'is': 1, 'the': 2, 'document': 3, '.': 4, 'first': 5}\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def create_vocabulary(texts, min_freq=2, max_vocab_size=10000):\n",
    "    # Tokenize all texts\n",
    "    all_tokens = []\n",
    "    for text in texts:\n",
    "        tokens = nltk.word_tokenize(text.lower())\n",
    "        all_tokens.extend(tokens)\n",
    "    \n",
    "    # Count frequency\n",
    "    token_counts = Counter(all_tokens)\n",
    "    \n",
    "    # Filter by frequency and vocabulary size\n",
    "    vocab = {token: count for token, count in token_counts.most_common(max_vocab_size) \n",
    "             if count >= min_freq}\n",
    "    \n",
    "    # Create mapping dictionaries\n",
    "    token2id = {token: idx for idx, (token, _) in enumerate(vocab.items())}\n",
    "    id2token = {idx: token for token, idx in token2id.items()}\n",
    "    \n",
    "    return vocab, token2id, id2token\n",
    "\n",
    "texts = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\",\n",
    "]\n",
    "\n",
    "vocab, token2id, id2token = create_vocabulary(texts, min_freq=2)\n",
    "print(\"Vocabulary:\", vocab)\n",
    "print(\"Token to ID mapping:\", token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ac3c69",
   "metadata": {},
   "source": [
    "Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98771f4",
   "metadata": {},
   "source": [
    "20. Comprehensive Preprocessing Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af9e8ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
