{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99994d97",
   "metadata": {},
   "source": [
    "# NMT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b183c7f5",
   "metadata": {},
   "source": [
    "Overview in Concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fc46a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder = 'part1'\n",
    "# decoder = 'part2'\n",
    "# # Traditional seq2seq (with bottleneck)\n",
    "# def translate_sentence(input_sentence):\n",
    "#     # Entire sentence compressed into one vector\n",
    "#     context_vector = encoder(input_sentence)\n",
    "    \n",
    "#     # Generate entire translation from just this vector\n",
    "#     translation = decoder(context_vector)\n",
    "#     return translation\n",
    "\n",
    "# # With Bahdanau attention (2014)\n",
    "# def translate_sentence_with_attention(input_sentence):\n",
    "#     # Encode input but keep all hidden states\n",
    "#     encoder_hidden_states = encoder(input_sentence)  # Shape: [input_length, hidden_size]\n",
    "    \n",
    "#     # For each output word\n",
    "#     translation = []\n",
    "#     for i in range(max_output_length):\n",
    "#         # Compute \"attention scores\" - how relevant is each input word?\n",
    "#         attention_scores = calculate_relevance(decoder_state, encoder_hidden_states)\n",
    "        \n",
    "#         # Create weighted sum of encoder states (attention context)\n",
    "#         context_vector = weighted_sum(attention_scores, encoder_hidden_states)\n",
    "        \n",
    "#         # Predict next word using both previous decoder state AND context\n",
    "#         next_word = decoder_step(decoder_state, context_vector)\n",
    "#         translation.append(next_word)\n",
    "    \n",
    "#     return translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e9a724",
   "metadata": {},
   "source": [
    "Use case on Opus Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82a7b030",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Amr osama abdellatif\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22bd9033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a04da72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded with 5000 examples\n",
      "Example 1:\n",
      "English: The Wanderer\n",
      "French: Le grand Meaulnes\n",
      "\n",
      "Example 2:\n",
      "English: Alain-Fournier\n",
      "French: Alain-Fournier\n",
      "\n",
      "Example 3:\n",
      "English: First Part\n",
      "French: PREMIÃˆRE PARTIE\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Amr osama abdellatif\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\marian\\tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "c:\\Users\\Amr osama abdellatif\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Load dataset and tokenizers\n",
    "# We'll use English-French from the Opus Books dataset\n",
    "dataset = load_dataset(\"opus_books\", \"en-fr\", split=\"train[:5000]\")  # Limit to 5000 examples for simplicity\n",
    "print(f\"Dataset loaded with {len(dataset)} examples\")\n",
    "\n",
    "# Sample a few examples\n",
    "for i in range(3):\n",
    "    print(f\"Example {i+1}:\")\n",
    "    print(f\"English: {dataset[i]['translation']['en']}\")\n",
    "    print(f\"French: {dataset[i]['translation']['fr']}\")\n",
    "    print()\n",
    "\n",
    "# Load tokenizers\n",
    "en_tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-fr\")\n",
    "fr_tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac28126b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([32, 50])\n",
      "Target shape: torch.Size([32, 50])\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Prepare dataset\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, dataset, en_tokenizer, fr_tokenizer, max_len=50):\n",
    "        self.dataset = dataset\n",
    "        self.en_tokenizer = en_tokenizer\n",
    "        self.fr_tokenizer = fr_tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get English and French sentences\n",
    "        en_text = self.dataset[idx]['translation']['en']\n",
    "        fr_text = self.dataset[idx]['translation']['fr']\n",
    "        \n",
    "        # Tokenize\n",
    "        en_tokens = self.en_tokenizer.encode(\n",
    "            en_text, \n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        ).squeeze(0)\n",
    "        \n",
    "        fr_tokens = self.fr_tokenizer.encode(\n",
    "            fr_text,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        ).squeeze(0)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': en_tokens,\n",
    "            'target_ids': fr_tokens,\n",
    "            'en_text': en_text,\n",
    "            'fr_text': fr_text\n",
    "        }\n",
    "\n",
    "# Create dataset and dataloader\n",
    "translation_dataset = TranslationDataset(dataset, en_tokenizer, fr_tokenizer)\n",
    "dataloader = DataLoader(translation_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Check a batch\n",
    "batch = next(iter(dataloader))\n",
    "print(f\"Input shape: {batch['input_ids'].shape}\")\n",
    "print(f\"Target shape: {batch['target_ids'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec51d7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Define the Encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hidden_dim, n_layers=1, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.GRU(emb_dim, hidden_dim, n_layers, bidirectional=True, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        # src: [batch_size, src_len]\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        # embedded: [batch_size, src_len, emb_dim]\n",
    "        \n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "        # outputs: [batch_size, src_len, hidden_dim * 2]\n",
    "        # hidden: [n_layers * 2, batch_size, hidden_dim]\n",
    "        \n",
    "        # Combine bidirectional outputs\n",
    "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "        # hidden: [batch_size, hidden_dim * 2]\n",
    "        \n",
    "        hidden = torch.tanh(self.fc(hidden))\n",
    "        # hidden: [batch_size, hidden_dim]\n",
    "        \n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5251aac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Define the Attention Mechanism (Bahdanau attention)\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear((hidden_dim * 2) + hidden_dim, hidden_dim)\n",
    "        self.v = nn.Linear(hidden_dim, 1, bias=False)\n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        # hidden: [batch_size, hidden_dim]\n",
    "        # encoder_outputs: [batch_size, src_len, hidden_dim * 2]\n",
    "        \n",
    "        batch_size = encoder_outputs.shape[0]\n",
    "        src_len = encoder_outputs.shape[1]\n",
    "        \n",
    "        # Repeat decoder hidden state src_len times\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        # hidden: [batch_size, src_len, hidden_dim]\n",
    "        \n",
    "        # Calculate energy\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
    "        # energy: [batch_size, src_len, hidden_dim]\n",
    "        \n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        # attention: [batch_size, src_len]\n",
    "        \n",
    "        # Apply softmax to get attention weights summing to 1\n",
    "        attention_weights = torch.softmax(attention, dim=1)\n",
    "        # attention_weights: [batch_size, src_len]\n",
    "        \n",
    "        # Get weighted sum of encoder outputs\n",
    "        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs)\n",
    "        # context: [batch_size, 1, hidden_dim * 2]\n",
    "        context = context.squeeze(1)\n",
    "        # context: [batch_size, hidden_dim * 2]\n",
    "        \n",
    "        return context, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "872b7560",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hidden_dim, attention, n_layers=1, dropout=0.5):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        self.attention = attention\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.n_layers = n_layers  # Add this line to store n_layers\n",
    "        \n",
    "        self.rnn = nn.GRU((hidden_dim * 2) + emb_dim, hidden_dim, n_layers, batch_first=True)\n",
    "        self.fc_out = nn.Linear((hidden_dim * 2) + hidden_dim + emb_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        # input: [batch_size, 1] or [batch_size]\n",
    "        # hidden: [batch_size, hidden_dim]\n",
    "        # encoder_outputs: [batch_size, src_len, hidden_dim * 2]\n",
    "        \n",
    "        if input.dim() == 1:\n",
    "            input = input.unsqueeze(1)  # Add sequence dimension if needed\n",
    "        # input: [batch_size, 1]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        # embedded: [batch_size, 1, emb_dim]\n",
    "        \n",
    "        # Calculate attention\n",
    "        context, attn_weights = self.attention(hidden, encoder_outputs)\n",
    "        # context: [batch_size, hidden_dim * 2]\n",
    "        # attn_weights: [batch_size, src_len]\n",
    "        \n",
    "        # Concatenate context and embedded\n",
    "        rnn_input = torch.cat((embedded, context.unsqueeze(1)), dim=2)\n",
    "        # rnn_input: [batch_size, 1, (hidden_dim * 2) + emb_dim]\n",
    "        \n",
    "        # Important change here: Reshape hidden to match the expected dimensions\n",
    "        # The RNN expects hidden state shape: [n_layers, batch_size, hidden_dim]\n",
    "        hidden = hidden.unsqueeze(0).repeat(self.n_layers, 1, 1)\n",
    "        # hidden: [n_layers, batch_size, hidden_dim]\n",
    "        \n",
    "        output, hidden = self.rnn(rnn_input, hidden)\n",
    "        # output: [batch_size, 1, hidden_dim]  \n",
    "        # hidden: [n_layers, batch_size, hidden_dim]\n",
    "        \n",
    "        # Take the last layer's hidden state\n",
    "        hidden_for_output = hidden[-1]\n",
    "        \n",
    "        output = output.squeeze(1)\n",
    "        # output: [batch_size, hidden_dim]\n",
    "        \n",
    "        # Concatenate for prediction\n",
    "        output = torch.cat((output, context, embedded.squeeze(1)), dim=1)\n",
    "        # output: [batch_size, (hidden_dim * 2) + hidden_dim + emb_dim]\n",
    "        \n",
    "        prediction = self.fc_out(output)\n",
    "        # prediction: [batch_size, output_dim]\n",
    "        \n",
    "        return prediction, hidden_for_output, attn_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c380bdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Define the Seq2Seq model with Attention\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        # src: [batch_size, src_len]\n",
    "        # trg: [batch_size, trg_len]\n",
    "        \n",
    "        batch_size = src.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        # Tensor to store decoder outputs\n",
    "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        # Tensor to store attention\n",
    "        attentions = torch.zeros(batch_size, trg_len, src.shape[1]).to(self.device)\n",
    "        \n",
    "        # Encode the source sequence\n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "        \n",
    "        # First input to the decoder is the <sos> token\n",
    "        input = trg[:, 0]\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            # Use previous hidden state to produce a new output\n",
    "            output, hidden, attention = self.decoder(input, hidden, encoder_outputs)\n",
    "            \n",
    "            # Store predictions and attention\n",
    "            outputs[:, t] = output\n",
    "            attentions[:, t] = attention\n",
    "            \n",
    "            # Decide whether to use teacher forcing\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            # Get the highest predicted token from our predictions\n",
    "            top1 = output.argmax(1)\n",
    "            \n",
    "            # If teacher forcing, use actual next token as next input\n",
    "            # If not, use predicted token\n",
    "            input = trg[:, t] if teacher_force else top1\n",
    "            \n",
    "        return outputs, attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8eec7a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 149,913,722 trainable parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Amr osama abdellatif\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\onnxruntime\\capi\\onnxruntime_validation.py:26: UserWarning: Unsupported Windows version (11). ONNX Runtime supports Windows 10 and above, only.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Initialize the model\n",
    "# Define hyperparameters\n",
    "INPUT_DIM = len(en_tokenizer.get_vocab())\n",
    "OUTPUT_DIM = len(fr_tokenizer.get_vocab())\n",
    "EMB_DIM = 256\n",
    "HIDDEN_DIM = 512\n",
    "N_LAYERS = 2\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "\n",
    "# Create model components\n",
    "encoder = Encoder(INPUT_DIM, EMB_DIM, HIDDEN_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "attention = Attention(HIDDEN_DIM)\n",
    "decoder = Decoder(OUTPUT_DIM, EMB_DIM, HIDDEN_DIM, attention, N_LAYERS, DEC_DROPOUT)\n",
    "\n",
    "# Create Seq2Seq model\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "\n",
    "# Initialize weights\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "            \n",
    "model.apply(init_weights)\n",
    "\n",
    "# Define optimizer and loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding index\n",
    "\n",
    "print(f\"The model has {sum(p.numel() for p in model.parameters() if p.requires_grad):,} trainable parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0536629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Define training function\n",
    "def train(model, dataloader, optimizer, criterion, clip=1):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for batch in tqdm(dataloader):\n",
    "        # Get data\n",
    "        src = batch['input_ids'].to(device)\n",
    "        trg = batch['target_ids'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        output, _ = model(src, trg)\n",
    "        \n",
    "        # Output: [batch_size, trg_len, output_dim]\n",
    "        # Trg: [batch_size, trg_len]\n",
    "        \n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        # Reshape output and target for loss calculation\n",
    "        output = output[:, 1:].reshape(-1, output_dim)  # Skip <sos> token\n",
    "        trg = trg[:, 1:].reshape(-1)  # Skip <sos> token\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(output, trg)\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # Clip gradients to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93ba549c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Define function to translate a sentence\n",
    "def translate_sentence(sentence, src_tokenizer, trg_tokenizer, model, device, max_len=50):\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize the source sentence\n",
    "    tokens = src_tokenizer.encode(sentence, return_tensors='pt').to(device)\n",
    "    \n",
    "    # Encode the source sentence\n",
    "    encoder_outputs, hidden = model.encoder(tokens)\n",
    "    \n",
    "    # Start with SOS token\n",
    "    input = torch.tensor([trg_tokenizer.bos_token_id]).to(device)\n",
    "    \n",
    "    trg_indexes = [trg_tokenizer.bos_token_id]\n",
    "    attentions = torch.zeros(max_len, tokens.shape[1]).to(device)\n",
    "    \n",
    "    for i in range(max_len):\n",
    "        # Decode one token at a time\n",
    "        output, hidden, attention = model.decoder(input, hidden, encoder_outputs)\n",
    "        \n",
    "        # Store attention scores\n",
    "        attentions[i] = attention\n",
    "        \n",
    "        # Get most likely next token\n",
    "        pred_token = output.argmax(1).item()\n",
    "        \n",
    "        # If EOS token, stop generating\n",
    "        if pred_token == trg_tokenizer.eos_token_id:\n",
    "            break\n",
    "        \n",
    "        # Add token to output\n",
    "        trg_indexes.append(pred_token)\n",
    "        \n",
    "        # Update input for next time step\n",
    "        input = torch.tensor([pred_token]).to(device)\n",
    "    \n",
    "    # Convert tokens to words\n",
    "    trg_tokens = trg_tokenizer.decode(trg_indexes)\n",
    "    \n",
    "    return trg_tokens, attentions[:i+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54fb6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Train for a few epochs (reduced for demonstration)\n",
    "N_EPOCHS = 3\n",
    "best_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    print(f\"Epoch {epoch+1}/{N_EPOCHS}\")\n",
    "    \n",
    "    train_loss = train(model, dataloader, optimizer, criterion)\n",
    "    \n",
    "    print(f\"\\tTrain Loss: {train_loss:.4f}\")\n",
    "    \n",
    "    # Save model if it has the best loss\n",
    "    if train_loss < best_loss:\n",
    "        best_loss = train_loss\n",
    "        torch.save(model.state_dict(), 'nmt-model.pt')\n",
    "        print(f\"\\tModel saved with loss: {best_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
